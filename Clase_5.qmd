---
title: "Clase 5:Limpieza y transformaci√≥n de datos con pandas en Python"
author: "Paul Yungan"
format: html
editor: visual
---

# Limpieza y transformaci√≥n de datos con pandas en Python

En el universo de la ciencia de datos, el manejo de datos crudos es el primer desaf√≠o que enfrentamos. Aqu√≠ es donde Pandas, la biblioteca de Python, se convierte en nuestra aliada clave. Este cap√≠tulo se sumerge en la esencia de la limpieza y transformaci√≥n de datos, habilidades fundamentales para cualquier an√°lisis significativo.

La importancia radica en que la limpieza de datos sienta las bases para cualquier an√°lisis. Pandas no solo facilita la eliminaci√≥n de imperfecciones como valores nulos y duplicados, sino que tambi√©n ofrece herramientas para transformar datos, adapt√°ndolos a nuestras necesidades anal√≠ticas.

Este cap√≠tulo no solo aborda lo t√©cnico, sino tambi√©n la importancia estrat√©gica de la limpieza en todo el ciclo de vida de un proyecto de ciencia de datos. Desde la adquisici√≥n de datos hasta la preparaci√≥n para modelos, estas habilidades son vitales en cada etapa del camino.

## ¬øSab√≠as que....?
¬øSab√≠as que el nombre "Pandas" proviene de las palabras "Panel Data"? Esta biblioteca fue creada por Wes McKinney en 2008 con el prop√≥sito de proporcionar herramientas de an√°lisis de datos flexibles y de alto rendimiento. Desde entonces, Pandas se ha convertido en un pilar fundamental en la caja de herramientas de cualquier cient√≠fico de datos que se precie.

### Objetivos de hoy:

- Dominaremos las Habilidades B√°sicas de Limpieza: Comprenderemos y aplicaremos t√©cnicas esenciales de limpieza de datos, como la identificaci√≥n y manejo de valores faltantes, duplicados y at√≠picos.
- Explorararemos M√©todos de Transformaci√≥n: Aprenderemos a transformar datos para adaptarlos a nuestras necesidades anal√≠ticas, ya sea mediante la modificaci√≥n de tipos de datos, la creaci√≥n de nuevas variables o la combinaci√≥n de conjuntos de datos.

¬°Prep√°rate para explorar el poder de Pandas y transformar tus datos en conocimiento valioso!

```{python}
# importante instalar : 
##pip install pandas pyreadstat
import pandas as pd
#import pyreadstat
```
Trabajaremos con la data 'EMPRESAS2021_periodo_2012_2021.sav'.
```{python}
df = pd.read_spss (r"data/directorio_empresas\data_original/EMPRESAS2021_periodo_2012_2021.sav")
df.head(10)
```

Esta data corresponde a datos de La Encuesta Estructural Empresarial 2019.

```{python}
df.shape
```
Recodifiquemos las variables y verificamos los tipos de variables
## Recodificaci√≥n de variables y creaci√≥n de nuevas variables a partir de datos existentes utilizando pandas. 

1. definiremos el datos
Identificamos el tipo de datos que tiene nuestra tabla de datos.   
```{python}
df.info()
```

```{python}
##tipos category 
print(df.select_dtypes(include=['category']).columns)
```

```{python}
##tipos category 
print(df.select_dtypes(include=['object']).columns)
```

```{python}
##tipos category 
print(df.select_dtypes(include=['float64']).columns)
```
Como detectamos la variabale id_empresa esta python le detecto como un n√∫mero, ahora le definiremos como una variable categorica, en este caso le definimos de 'str' (cadena):

```{python}
df['id_empresa'] = df['id_empresa'].astype(str)
print (df['id_empresa'].dtype)
```

Como ejemplo,  si necesitamos cambiar el tipo de dato de multiples variables podemos reaizarlo con la siguiente linea de c√≥digo:
'df = df.astype({'variable1': float64, 'variable2': str, 'variable3': int ...})'

## Limpieza de datos, tratamiento de valores vac√≠os y duplicados en Python. 

En el √°mbito econ√≥mico y de la ciencia de datos, la calidad de los datos es esencial para la toma de decisiones informadas y el desarrollo de modelos precisos. En esta secci√≥n, nos sumergiremos en el proceso de limpieza de datos utilizando la biblioteca Pandas en Python, una herramienta omnipresente en la comunidad de an√°lisis de datos.

Exploraremos estrategias espec√≠ficas para abordar valores vac√≠os y duplicados, destacando las capacidades de Pandas para gestionar estos desaf√≠os de manera eficiente. Desde la identificaci√≥n de celdas nulas hasta la detecci√≥n y eliminaci√≥n de registros duplicados, aprenderemos a utilizar las funciones clave de Pandas para mejorar la calidad de los conjuntos de datos.

A lo largo de ejemplos pr√°cticos y aplicaciones en contextos econ√≥micos y cient√≠ficos de datos, proporcionaremos insights valiosos sobre c√≥mo aprovechar al m√°ximo Pandas para optimizar la preparaci√≥n de datos. Estas habilidades son fundamentales para garantizar que los economistas y cient√≠ficos de datos puedan realizar an√°lisis robustos y fundamentar sus conclusiones en datos confiables.

Detectamos de nuestra tabla de datos valores nulos o vacios por variable.
```{python}
##creamos una df para saber cuantas NAs hay por vaiable y su porcentaje
var_missing = df.isna().sum()

tabla_missing = pd.DataFrame({
    'variable': var_missing.index,
    'num_nulos': var_missing.values,
    'porc_nulos' : (var_missing/len(df))
    })
print (tabla_missing)
```
En la anterior tabla mostramos los datos perdidos o NA por variable y su respectivo porcentaje del total de observaciones. 

Para el ejercicio de elegiremos las variables :
- anio
- forma_institucional
- tamano_cop
- gsectores 
-propietarios_sexo
- ventas_totales
-exportaciones_netas
- ventas_nacionales

```{python}
df_1 = df[[ 'anio', 'forma_institucional', 'tamano_cop', 'gsectores', 'propietarios_sexo', 'ventas_totales', 'exportaciones_netas', 'ventas_nacionales']]
print (df_1.head(5))
df_1.shape
```
## M√©todos de Imputaci√≥n

n el proceso de an√°lisis de datos, es frecuente enfrentarse a conjuntos de datos que presentan valores faltantes, los cuales requieren ser tratados para llevar a cabo un an√°lisis o modelado preciso. Arreglar esto es clave para hacer an√°lisis o modelos precisos. Para llenar esos huecos, usamos trucos llamados t√©cnicas de imputaci√≥n, que b√°sicamente son maneras de poner valores en los espacios vac√≠os. Pero aqu√≠ est√° el truco: NO hay una soluci√≥n √∫nica que funcione para todos los casos. ¬øPor qu√©? Pues, porque la mejor forma de lidiar con esos vac√≠os depende de por qu√© se fueron o de d√≥nde vinieron. No todos los m√©todos valen para todo. Entender la raz√≥n detr√°s de esos huecos es clave para elegir la mejor estrategia y conseguir resultados confiables. ¬°As√≠ que, a llenar esos huecos de manera inteligente! üòâ‚ú®

### Imputaci√≥n por Media/Mediana/Moda:
- Cu√°ndo Usar: Ideal para variables num√©ricas con distribuciones no sesgadas. R√°pido y simple.
- Ventajas: Simple y r√°pida. Apropiada para variables num√©ricas.
- Consideraciones: Puede no ser apropiado si los datos tienen sesgo o distribuciones at√≠picas.

```{python}
# Ejemplo de imputaci√≥n por media
df['columna'].fillna(df['columna'].mean(), inplace=True)

```

### Imputaci√≥n por Vecinos M√°s Cercanos (KNN):
Imputaci√≥n por Vecinos M√°s Cercanos (KNN)
- Cu√°ndo Usar: √ötil cuando los valores faltantes est√°n relacionados con valores cercanos en el espacio de caracter√≠sticas.
- Ventajas: Utiliza informaci√≥n de variables relacionadas. √ötil para datos con patrones complejos.
- Consideraciones: Puede ser computacionalmente costoso.

```{python}
from sklearn.impute import KNNImputer
imputer = KNNImputer(n_neighbors=2)
df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)

```
### Imputaci√≥n por regresi√≥n
- Cu√°ndo Usar: Efectivo cuando existe una relaci√≥n lineal entre las variables. Adecuado para variables continuas.
- Ventajas: Utiliza relaciones lineales entre variables.
- Consideraciones: Requiere que las variables predictoras est√©n correlacionadas con la variable a imputar.

```{python}
from sklearn.linear_model import LinearRegression
# Supongamos que 'target' es la variable a imputar
model = LinearRegression()
known_data = df.dropna(subset=['target'])
model.fit(known_data[['feature1', 'feature2']], known_data['target'])
df.loc[df['target'].isna(), 'target'] = model.predict(df.loc[df['target'].isna(), ['feature1', 'feature2']])

```

### Imputaci√≥n M√∫ltiple (MICE)

- Cu√°ndo Usar: √ötil cuando se desea considerar la incertidumbre en el proceso de imputaci√≥n. Adecuado para conjuntos de datos complejos.
- Ventajas: Considera la incertidumbre. Puede utilizarse para imputar m√∫ltiples variables simult√°neamente
- Consideraciones: Puede ser computacionalmente intensivo y puede requerir ajustes adicionales.

```{python}
from impyute.imputation.cs import mice
df_imputed = pd.DataFrame(mice(df.values), columns=df.columns)
```

## Normalizaci√≥n y estandarizaci√≥n de datos en Python.


## Las partes de un objeto `DataFrame` de pandas como clave (`key`) e √≠ndice (`index`) en el contexto de series de tiempo. 