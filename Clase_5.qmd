---
title: "Clase 5:Limpieza y transformaci√≥n de datos con pandas en Python"
author: "Paul Yungan"
format: html
editor: visual
---

# Limpieza y transformaci√≥n de datos con pandas en Python

En el universo de la ciencia de datos, el manejo de datos crudos es el primer desaf√≠o que enfrentamos. Aqu√≠ es donde Pandas, la biblioteca de Python, se convierte en nuestra aliada clave. Este cap√≠tulo se sumerge en la esencia de la limpieza y transformaci√≥n de datos, habilidades fundamentales para cualquier an√°lisis significativo.

La importancia radica en que la limpieza de datos sienta las bases para cualquier an√°lisis. Pandas no solo facilita la eliminaci√≥n de imperfecciones como valores nulos y duplicados, sino que tambi√©n ofrece herramientas para transformar datos, adapt√°ndolos a nuestras necesidades anal√≠ticas.

Este cap√≠tulo no solo aborda lo t√©cnico, sino tambi√©n la importancia estrat√©gica de la limpieza en todo el ciclo de vida de un proyecto de ciencia de datos. Desde la adquisici√≥n de datos hasta la preparaci√≥n para modelos, estas habilidades son vitales en cada etapa del camino.

## ¬øSab√≠as que....?
¬øSab√≠as que el nombre "Pandas" proviene de las palabras "Panel Data"? Esta biblioteca fue creada por Wes McKinney en 2008 con el prop√≥sito de proporcionar herramientas de an√°lisis de datos flexibles y de alto rendimiento. Desde entonces, Pandas se ha convertido en un pilar fundamental en la caja de herramientas de cualquier cient√≠fico de datos que se precie.

### Objetivos de hoy:

- Dominaremos las Habilidades B√°sicas de Limpieza: Comprenderemos y aplicaremos t√©cnicas esenciales de limpieza de datos, como la identificaci√≥n y manejo de valores faltantes, duplicados y at√≠picos.
- Explorararemos M√©todos de Transformaci√≥n: Aprenderemos a transformar datos para adaptarlos a nuestras necesidades anal√≠ticas, ya sea mediante la modificaci√≥n de tipos de datos, la creaci√≥n de nuevas variables o la combinaci√≥n de conjuntos de datos.

¬°Prep√°rate para explorar el poder de Pandas y transformar tus datos en conocimiento valioso!

```{python}
# importante instalar : 
##pip install pandas pyreadstat
import pandas as pd
#import pyreadstat
from IPython.display import display
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.impute import KNNImputer
from sklearn.linear_model import LinearRegression
from impyute.imputation.cs import mice
import missingno as msno


```
Trabajaremos con la data 'EMPRESAS2021_periodo_2012_2021.sav'.
```{python}
df = pd.read_spss (r"data/directorio_empresas\data_original/EMPRESAS2021_periodo_2012_2021.sav")
df.head(10)
```

Esta data corresponde a datos de La Encuesta Estructural Empresarial 2019.

```{python}
df.shape
```
Recodifiquemos las variables y verificamos los tipos de variables
## Recodificaci√≥n de variables y creaci√≥n de nuevas variables a partir de datos existentes utilizando pandas. 

1. definiremos el datos
Identificamos el tipo de datos que tiene nuestra tabla de datos.   
```{python}
df.info()
```

```{python}
##tipos category 
print(df.select_dtypes(include=['category']).columns)
```

```{python}
##tipos object 
print(df.select_dtypes(include=['object']).columns)
```

```{python}
##tipos float64 
print(df.select_dtypes(include=['float64']).columns)
```
Como detectamos la variabale id_empresa esta python le detecto como un n√∫mero, ahora le definiremos como una variable categorica, en este caso le definimos de 'str' (cadena):

```{python}
df['id_empresa'] = df['id_empresa'].astype(str)
print (df['id_empresa'].dtype)
```

Como ejemplo,  si necesitamos cambiar el tipo de dato de multiples variables podemos reaizarlo con la siguiente linea de c√≥digo:
'df = df.astype({'variable1': float64, 'variable2': str, 'variable3': int ...})'

## Limpieza de datos, tratamiento de valores vac√≠os y duplicados en Python. 

En el √°mbito econ√≥mico y de la ciencia de datos, la calidad de los datos es esencial para la toma de decisiones informadas y el desarrollo de modelos precisos. En esta secci√≥n, nos sumergiremos en el proceso de limpieza de datos utilizando la biblioteca Pandas en Python, una herramienta omnipresente en la comunidad de an√°lisis de datos.

Exploraremos estrategias espec√≠ficas para abordar valores vac√≠os y duplicados, destacando las capacidades de Pandas para gestionar estos desaf√≠os de manera eficiente. Desde la identificaci√≥n de celdas nulas hasta la detecci√≥n y eliminaci√≥n de registros duplicados, aprenderemos a utilizar las funciones clave de Pandas para mejorar la calidad de los conjuntos de datos.

A lo largo de ejemplos pr√°cticos y aplicaciones en contextos econ√≥micos y cient√≠ficos de datos, proporcionaremos insights valiosos sobre c√≥mo aprovechar al m√°ximo Pandas para optimizar la preparaci√≥n de datos. Estas habilidades son fundamentales para garantizar que los economistas y cient√≠ficos de datos puedan realizar an√°lisis robustos y fundamentar sus conclusiones en datos confiables.

Detectamos de nuestra tabla de datos valores nulos o vacios por variable.
```{python}
##creamos una df para saber cuantas NAs hay por vaiable y su porcentaje
var_missing = df.isna().sum()

tabla_missing = pd.DataFrame({
    'variable': var_missing.index,
    'num_nulos': var_missing.values,
    'porc_nulos' : (var_missing/len(df))
    })
pd.set_option('display.max_rows', None) 
display(tabla_missing)
```
En la anterior tabla mostramos los datos perdidos o NA por variable y su respectivo porcentaje del total de observaciones. 

Para el ejercicio de elegiremos, por simplicidad las variables :
- id_empresa
- anio
- forma_institucional
- tamano_cop
- gsectores 
-propietarios_sexo
- ventas_totales
-exportaciones_netas
- ventas_nacionales

```{python}
df_1 = df[[ 'id_empresa','anio', 'forma_institucional', 'tamano_cop', 'gsectores', 'propietarios_sexo', 'ventas_totales', 'exportaciones_netas', 'ventas_nacionales']]
print (df_1.head(5))
df_1.shape

df_1 = df.groupby(['id_empresa', 'anio', 'forma_institucional', 'tamano_cop', 'gsectores', 'propietarios_sexo']).agg({
    'ventas_totales': 'sum',
    'exportaciones_netas': 'sum',
    'ventas_nacionales': 'sum'
}).reset_index()

df_1 = df.groupby (['id_empres', 'anio','forma_institucional','tamano_cop', 'gsectores', 'propietarios_sexo']).agg({
    'ventas_totales': 'sum',
    
})

```
## An√°lisis de datos faltantes
En el proceso de an√°lisis de datos, es frecuente enfrentarse a conjuntos de datos que presentan valores faltantes, los cuales requieren ser tratados para llevar a cabo un an√°lisis o modelado preciso. Arreglar esto es clave para hacer an√°lisis o modelos precisos. Para llenar esos huecos, usamos trucos llamados t√©cnicas de imputaci√≥n, que b√°sicamente son maneras de poner valores en los espacios vac√≠os. Pero aqu√≠ est√° el truco: NO hay una soluci√≥n √∫nica que funcione para todos los casos. ¬øPor qu√©? Pues, porque la mejor forma de lidiar con esos vac√≠os depende de por qu√© se fueron o de d√≥nde vinieron. No todos los m√©todos valen para todo. Entender la raz√≥n detr√°s de esos huecos es clave para elegir la mejor estrategia y conseguir resultados confiables. ¬°As√≠ que, a llenar esos huecos de manera inteligente! üòâ‚ú®

Para los siguientes ejemplos de imputaci√≥n revisaremos los datos perdidos pero iniciamos con un analisis univariado de las variables continuas.

Revisamos atravez de del siguiente grafico los datos faltantes por variable de nuestra dataset:

```{python}

msno.bar(df_1,figsize=(12, 6), sort="ascending",fontsize=12, color='tomato') 
```

Tambien Podemos visualizar las posiciones en el dataset de los datos faltantes usando la matrix(). El gr√°fico aparece en blanco siempre que falten valores.

```{python}
msno.matrix(df_1,figsize=(12, 6), fontsize=12, color=[0,0,0.2])
```

### Razones  para tener datos faltantes

#### 1.Missing Completely at Random (MCAR)
- Los valores faltantes de una variable dada (Y) no estan asociados con otras variables del dataset dado o con la misma variable (Y).
- Se refiere a la situaci√≥n en la que la falta de datos ocurre de manera totalmente aleatoria, sin relaci√≥n con ninguna variable observada o no observada. En otras palabras, la probabilidad de que falten datos es la misma para todas las unidades, sin importar sus valores o la naturaleza de los datos faltantes.

Ejemplo:

Supongamos que estamos recopilando datos sobre los ingresos mensuales de los empleados en una empresa. Si algunas personas olvidan proporcionar sus ingresos debido a distracciones casuales o simplemente a errores administrativos, y este olvido no est√° relacionado con la cantidad de ingresos que tienen ni con ninguna otra caracter√≠stica observada, entonces estamos en una situaci√≥n de Missing Completely at Random (MCAR).

En este escenario, la falta de datos (no informar los ingresos) ocurre al azar, sin depender de ninguna variable espec√≠fica, y podr√≠a ser tan probable para un empleado con ingresos bajos como para uno con ingresos altos. Este comportamiento aleatorio de la falta de datos caracteriza la condici√≥n de MCAR.

#### 2. Missing at Random (MAR)
- MAR ocurre la p√©rdida de los datos no es aleatoria, pero puede ser explicada completamente como funcion de otras variables con informacion completa.
- Se refiere a la situaci√≥n en la que la falta de datos puede depender de variables observadas, pero no de los valores reales de los datos faltantes. En otras palabras, la probabilidad de que falten datos puede estar relacionada con otras variables conocidas, pero no directamente con la informaci√≥n que est√° ausente.

Ejemplo:

Imagina que est√°s estudiando los patrones de gasto en compras en l√≠nea. La probabilidad de que alguien no proporcione sus datos de gasto puede depender de una variable observada, como el n√∫mero de transacciones realizadas en el √∫ltimo mes, pero no directamente del monto real gastado.

Por ejemplo, aquellos que realizan muchas transacciones podr√≠an ser menos propensos a informar cada una de ellas. Aunque la falta de datos (no informar el gasto) est√° relacionada con una variable observada (n√∫mero de transacciones), no est√° directamente vinculada con la cantidad exacta de gasto en compras en l√≠nea. En este caso, estar√≠amos en una situaci√≥n de Missing at Random (MAR), ya que la falta de datos est√° relacionada con una variable observada pero no con la informaci√≥n faltante en s√≠ misma.

#### 3. Missing Not at Random (MNAR)
- La p√©rdida depende de datos no observados o del valor perdido.
- MNAR (Missing Not at Random) se refiere a la situaci√≥n en la que la falta de datos est√° relacionada directamente con la informaci√≥n que est√° ausente. En este caso, la probabilidad de que falten datos depende de los valores reales no observados. Existen patrones sistem√°ticos en la ausencia de datos que no pueden explicarse por variables observadas.

Ejemplo:

Imagina que est√°s llevando a cabo una encuesta sobre los niveles de endeudamiento de las personas. Sin embargo, resulta que aquellos con deudas m√°s altas son m√°s propensos a ocultar sus cifras reales. En este caso, la falta de datos (no informar la deuda) est√° directamente relacionada con la informaci√≥n que est√° ausente (deudas elevadas).

Las personas con deudas significativas podr√≠an sentirse inc√≥modas al divulgar la totalidad de su endeudamiento y, por lo tanto, tienen m√°s probabilidades de no proporcionar la informaci√≥n precisa. La falta de datos no puede explicarse f√°cilmente por variables observadas; est√° intr√≠nsecamente vinculada a la magnitud real de la deuda. Este escenario representa un ejemplo de Missing Not at Random (MNAR).



## M√©todos de Imputaci√≥n

Para este caso, como primer proceso se realizaremos la eliminanci√≥n de valores perdidos en comun entre las variables 

```{python}
msno.bar(melb_data,figsize=(12, 6), sort="ascending",fontsize=12, color='tomato') 
```


```{python}
## df para pruebas
df_2 = df_1

print (
    pd.DataFrame ({
    'variable': df_2.isna().sum().index,
    'num_nulos': df_2.isna().sum().values,
    'porc_nulos':df_2.isna().sum()/ len(df_2)
    })
)
```

```{python}
sns.set(style="ticks")
sns.pairplot(df_2[['ventas_totales', 'exportaciones_netas', 'ventas_nacionales']], height=2, diag_kind="hist", diag_kws=dict(bins=20))
plt.show()
```
Observamos que los datos presentan una concentraci√≥n notable o un sesgo hacia la derecha. Para contrarrestar este efecto, podemos aplicar ingenier√≠a de variables, lo que nos permitir√° suavizar la distribuci√≥n. Una t√©cnica com√∫nmente utilizada es aplicar el logaritmo a la variable m√°s 1.

```{python}

df_2[['log_ventas_totales', 'log_exportaciones_netas', 'log_ventas_nacionales']] = np.log1p(df_2[['ventas_totales', 'exportaciones_netas', 'ventas_nacionales']])

print(df_2.head(5))
```  

```{python}
sns.set(style="ticks")
sns.pairplot(df_2[['log_ventas_totales', 'log_exportaciones_netas', 'log_ventas_nacionales']], height=2, diag_kind="hist", diag_kws=dict(bins=20))
plt.show()


```

```{python}
df_2.describe
```
```{python}

df_2.shape
```

A pesar de haber suavizado la distribuci√≥n de los datos, notamos una presencia notable de valores iguales a cero. En este contexto, con el fin de facilitar el an√°lisis, procederemos a eliminar las observaciones que presentan un valor de cero en las tres variables que estamos estudiando.

```{python}
df_2 = df_2.loc[(df_2[['log_ventas_totales', 'log_exportaciones_netas', 'log_ventas_nacionales']] == 0).all(axis=1)]

print(df_2.head(5))
```

```{python}
df_2.shape
```

```{python}
df_2.describe
```

```{python}
sns.set(style="ticks")
sns.pairplot(df_2[['log_ventas_totales', 'log_exportaciones_netas', 'log_ventas_nacionales']], height=2, diag_kind="hist", diag_kws=dict(bins=20))
plt.show()

```

Ahora, tras realizar esta breve pero esencial limpieza de datos, procedemos a examinar el porcentaje de valores nulos en esta tabla de datos depurada.

```{python}
print (
    pd.DataFrame ({
    'variable': df_2.isna().sum().index,
    'num_nulos': df_2.isna().sum().values,
    'porc_nulos':df_2.isna().sum()/ len(df_2)
    })
)
```

```{python}
##funcion para generar una taba summary
def tabla_summ(df, variables):
    summary = pd.DataFrame(index=variables)
    summary['Count'] = df[variables].count()
    summary['Mean'] = df[variables].mean()
    summary['Std Dev'] = df[variables].std()
    summary['Min'] = df[variables].min()
    summary['25%'] = df[variables].quantile(0.25)
    summary['50%'] = df[variables].median()
    summary['75%'] = df[variables].quantile(0.75)
    summary['Max'] = df[variables].max()
    summary['Missing'] = df[variables].isnull().sum()
    summary['Missing %'] = (summary['Missing'] / len(df)) 
   
    return summary
```

```{python}
var = ['ventas_totales',  'exportaciones_netas','ventas_nacionales','log_ventas_totales','log_exportaciones_netas','log_ventas_nacionales']
summary_result = tabla_summ(df_2, var)
display(summary_result)
```


### Imputaci√≥n por Media/Mediana/Moda:
- Cu√°ndo Usar: Ideal para variables num√©ricas con distribuciones no sesgadas. R√°pido y simple.
- Ventajas: Simple y r√°pida. Apropiada para variables num√©ricas.
- Consideraciones: Puede no ser apropiado si los datos tienen sesgo o distribuciones at√≠picas.


```{python}
# Ejemplo de imputaci√≥n por media
df2['columna'].fillna(df['columna'].mean(), inplace=True)

```

### Imputaci√≥n por Vecinos M√°s Cercanos (KNN):
Imputaci√≥n por Vecinos M√°s Cercanos (KNN)
- Cu√°ndo Usar: √ötil cuando los valores faltantes est√°n relacionados con valores cercanos en el espacio de caracter√≠sticas.
- Ventajas: Utiliza informaci√≥n de variables relacionadas. √ötil para datos con patrones complejos.
- Consideraciones: Puede ser computacionalmente costoso.

```{python}
imputer = KNNImputer(n_neighbors=2)
df_imputed = pd.DataFrame(imputer.fit_transform(df_2), columns=df_2.columns) ### revisaremos ojooo#######

```
### Imputaci√≥n por regresi√≥n
- Cu√°ndo Usar: Efectivo cuando existe una relaci√≥n lineal entre las variables. Adecuado para variables continuas.
- Ventajas: Utiliza relaciones lineales entre variables.
- Consideraciones: Requiere que las variables predictoras est√©n correlacionadas con la variable a imputar.

```{python}

# REVISARRRRRR!!! EJEMPLO : VAR_2 A IMPUTAR
model = LinearRegression()
known_data = df_2.dropna(subset=['VAR_2'])
model.fit(known_data[['feature1', 'feature2']], known_data['VAR_2'])
df.loc[df['VAR_2'].isna(), 'VAR_2'] = model.predict(df.loc[df['VAR_2'].isna(), ['VAR_2', 'VAR_2']])

```

### Imputaci√≥n M√∫ltiple (MICE)

- Cu√°ndo Usar: √ötil cuando se desea considerar la incertidumbre en el proceso de imputaci√≥n. Adecuado para conjuntos de datos complejos.
- Ventajas: Considera la incertidumbre. Puede utilizarse para imputar m√∫ltiples variables simult√°neamente
- Consideraciones: Puede ser computacionalmente intensivo y puede requerir ajustes adicionales.

```{python}

df_imputed = pd.DataFrame(mice(df.values), columns=df.columns)
```

## Normalizaci√≥n y estandarizaci√≥n de datos en Python.


## Las partes de un objeto `DataFrame` de pandas como clave (`key`) e √≠ndice (`index`) en el contexto de series de tiempo. 



```