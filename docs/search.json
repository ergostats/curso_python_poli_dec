[
  {
    "objectID": "index.html#clase-1-introducción-a-python-y-su-entorno-de-desarrollo",
    "href": "index.html#clase-1-introducción-a-python-y-su-entorno-de-desarrollo",
    "title": "Introducción al Lenguaje de Programación Python para Análisis Estadístico",
    "section": "Clase 1: Introducción a Python y su entorno de desarrollo",
    "text": "Clase 1: Introducción a Python y su entorno de desarrollo\n\nIntroducción a Python y su sintaxis básica.\nInstalación de paquetes en Python utilizando pip.\nUso de entornos virtuales para gestionar dependencias.\nFuentes primarias de consulta con Python, incluyendo la documentación oficial y la comunidad en línea.\nBuenas prácticas de programación en Python.\nComprendiendo el entorno de desarrollo de Python"
  },
  {
    "objectID": "index.html#clase-2-introducción-al-manejo-de-datos-en-python-con-pandas",
    "href": "index.html#clase-2-introducción-al-manejo-de-datos-en-python-con-pandas",
    "title": "Introducción al Lenguaje de Programación Python para Análisis Estadístico",
    "section": "Clase 2: Introducción al manejo de datos en Python con pandas",
    "text": "Clase 2: Introducción al manejo de datos en Python con pandas\n\nImportación de datos en Python desde diferentes fuentes utilizando la biblioteca pandas."
  },
  {
    "objectID": "index.html#clase-3-la-importancia-del-análisis-de-datos-y-reproducibilidad-con-funciones",
    "href": "index.html#clase-3-la-importancia-del-análisis-de-datos-y-reproducibilidad-con-funciones",
    "title": "Introducción al Lenguaje de Programación Python para Análisis Estadístico",
    "section": "Clase 3: La importancia del análisis de datos y reproducibilidad con funciones",
    "text": "Clase 3: La importancia del análisis de datos y reproducibilidad con funciones\n\nConceptos clave para el análisis efectivo de datos.\nIniciativas para abordar el temas sociales utilizando Python, como el uso de módulos como pandas em investigación académica."
  },
  {
    "objectID": "index.html#clase-4-principios-de-estadística-en-python",
    "href": "index.html#clase-4-principios-de-estadística-en-python",
    "title": "Introducción al Lenguaje de Programación Python para Análisis Estadístico",
    "section": "Clase 4: Principios de estadística en Python",
    "text": "Clase 4: Principios de estadística en Python\n\nConceptos estadísticos como variables aleatorias, funciones de distribución y medidas de tendencia central.\nAnálisis de variables numéricas utilizando módulos como scipy y pandas.\nAnálisis de variables categóricas con enfoque en la biblioteca pandas.\nOperaciones entre variables y cálculos estadísticos en Python."
  },
  {
    "objectID": "index.html#clase-5-limpieza-y-transformación-de-datos-con-pandas-en-python",
    "href": "index.html#clase-5-limpieza-y-transformación-de-datos-con-pandas-en-python",
    "title": "Introducción al Lenguaje de Programación Python para Análisis Estadístico",
    "section": "Clase 5: Limpieza y transformación de datos con pandas en Python",
    "text": "Clase 5: Limpieza y transformación de datos con pandas en Python\n\nRecodificación de variables y creación de nuevas variables a partir de datos existentes utilizando pandas.\nLimpieza de datos, tratamiento de valores vacíos y duplicados en Python.\nNormalización y estandarización de datos en Python.\nLas partes de un objeto DataFrame de pandas como clave (key) e índice (index) en el contexto de series de tiempo."
  },
  {
    "objectID": "index.html#clase-6-exploración-y-visualización-de-datos-con-matplotlib-y-seaborn-en-python",
    "href": "index.html#clase-6-exploración-y-visualización-de-datos-con-matplotlib-y-seaborn-en-python",
    "title": "Introducción al Lenguaje de Programación Python para Análisis Estadístico",
    "section": "Clase 6: Exploración y visualización de datos con Matplotlib y Seaborn en Python",
    "text": "Clase 6: Exploración y visualización de datos con Matplotlib y Seaborn en Python\n\nLa gramática de los gráficos utilizando bibliotecas como Matplotlib y Seaborn.\nVisualización de variables numéricas y categóricas con ejemplos prácticos en Python.\nPersonalización de gráficos, paletas de colores y etiquetas para mejorar la interpretación de los datos en Python."
  },
  {
    "objectID": "index.html#clase-7-pruebas-de-hipótesis-para-la-media-entre-grupos-en-python",
    "href": "index.html#clase-7-pruebas-de-hipótesis-para-la-media-entre-grupos-en-python",
    "title": "Introducción al Lenguaje de Programación Python para Análisis Estadístico",
    "section": "Clase 7: Pruebas de hipótesis para la media entre grupos en Python",
    "text": "Clase 7: Pruebas de hipótesis para la media entre grupos en Python\n\nIntroducción a las pruebas de hipótesis en Python.\nCasos de acuerdo a la varianza en los grupos (prueba t de Student) y casos de desbalance en los tamaños de los grupos.\nComparación y validación de estadísticas de grupo utilizando bibliotecas como SciPy."
  },
  {
    "objectID": "index.html#clase-8-análisis-de-correlación-y-modelos-de-regresión-en-python",
    "href": "index.html#clase-8-análisis-de-correlación-y-modelos-de-regresión-en-python",
    "title": "Introducción al Lenguaje de Programación Python para Análisis Estadístico",
    "section": "Clase 8: Análisis de correlación y modelos de regresión en Python",
    "text": "Clase 8: Análisis de correlación y modelos de regresión en Python\n\nEl modelo de mínimos cuadrados ordinarios con una sola variable en Python.\nInterpretación y validación de los parámetros de una regresión en Python.\nLa relación entre un modelo de regresión y la correlación de los datos en Python."
  },
  {
    "objectID": "index.html#clase-9-regresión-lineal-con-múltiples-variables-en-python",
    "href": "index.html#clase-9-regresión-lineal-con-múltiples-variables-en-python",
    "title": "Introducción al Lenguaje de Programación Python para Análisis Estadístico",
    "section": "Clase 9: Regresión lineal con múltiples variables en Python",
    "text": "Clase 9: Regresión lineal con múltiples variables en Python\n\nInclusión de variables dicotómicas en el modelo de regresión en Python.\nInterpretación de las estadísticas de rendimiento de los modelos de regresión, incluyendo R cuadrado y p-valores.\nSelección y comparación de modelos utilizando técnicas como el método de selección de características."
  },
  {
    "objectID": "index.html#clase-10-estudios-académicos",
    "href": "index.html#clase-10-estudios-académicos",
    "title": "Introducción al Lenguaje de Programación Python para Análisis Estadístico",
    "section": "Clase 10: Estudios académicos",
    "text": "Clase 10: Estudios académicos\n\nRevisión de la estructura de una investigación académica relacionada con la movilidad humana en Python.\nMétodos para abordar estadística desde la visualización y la disponibilidad de datos en Python.\nLecciones y métodos narrativos para abordar desde la estadística a la migración en Python."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introducción",
    "section": "",
    "text": "Las necesidades de formación de los ecuatorianos han cambiado drásticamente a raíz de la pandemia COVID-19. La era de la digitalización ha determinado que las habilidades digitales son hoy en día requisitos para el éxito en el mercado laboral, más no una garantía.\nAmérica Latina enfrenta un problema importante: el descalce de habilidades entre la oferta y la demanda laboral. Es decir, no existe un consenso entre las habilidades que se enseñan en las aulas y las necesidades que las empresas buscan suplir con las habilidades de sus trabajadores. El incremento del personal subcalificado en las distintas ocupaciones se suma al déficit de habilidades digitales. (Cepal, Seminario Regional de Desarrollo Social, 2023).\nLa formación en habilidades digitales representa un gran desafío para la academia en América Latina debido al descalce de habilidades entre la oferta y la demanda laboral. Ecuador presenta los porcentajes más bajos de adultos con altos niveles de logro en la resolución de problemas en ambientes digitales (5.3%) por debajo de países como Chile y Mexico (14.5% y 10.2%) que aun estando por encima de Ecuador, ilustran en conjunto la problemática que enfrenta la región. (CEPAL, base a datos PIAAC (Programa para la Evaluación Internacional de Competencias de Adultos), OCDE).\nPara superar esta problemática es necesario un cambio de enfoque en la manera en que se abordan las habilidades digitales y, por qué no, las habilidades que se imparten en las aulas. En primer lugar, se debe definir cuáles son las falencias de los sistemas educativos en los distintos niveles de educación acordes a cada etapa del desarrollo del individuo. Esta evaluación nos permitirá redireccionar la educación hacia un enfoque respetuoso, inclusivo y equitativo, que a su vez estén alineados con el sector productivo.\nQueda claro que superar esta problemática es una tarea que por definición debe ser articulada. Donde estudiantes, profesores, personal administrativo, gobierno y el sector productivo establezcan un dialogo proactivo en aras de garantizar una mejor calidad de la educación, el cierre de las brechas de oportunidades y la corrección del descalce entre la oferta y demanda laboral. Se han dado pasos importantes en esa dirección desde el sector gobierno del Ecuador al establecer un primer borrador sobre la calidad de evaluación de las habilidades digitales (INEVAL 2023). Así mismo, el crecimiento de la presencia centros de formación y capacitación dan cuenta de un avance paulatino hacía una sociedad con mayores oportunidades y beneficios sociales a partir de la educación.\nEs en este contexto, y en consonancia con las prioridades nacionales, el Centro de Investigación Estadística ERGOSTATS S.A.S (ERGOSTATS en adelante) determino como una de sus actividades principales la educación complementaria y alineada con los programas de formación académica oficiales de la educación superior del Ecuador.\nEn esta línea, desde 2021 ERGOSTATS se ha desempeñado en la planificación, ejecución y evaluación de cursos, talleres, webinars y charlas que fomenten la participación de la ciudadanía en espacios de aprendizaje sobre los usos, aplicaciones y experiencias en el análisis y visualización de datos, habilidades digitales altamente demandadas en el mercado laboral actual.\nA raíz de la experiencia de ERGOSTATS en el campo de la enseñanza complementaria se ha corroborado la presencia del descalce entre la oferta y demanda laborales y los desafíos que está representa para llevar a cabo la nivelación de los trabajadores hasta el nivel que exige el sector productivo. Un desafío adicional evidenciado por ERGOSTATS es la discriminación a través del precio, lo cual se ha convertido en una brecha de acceso para estudiantes que están cursando sus primeras etapas en la educación superior.\nEn vista de lo mencionado, y contemplando que la educación en términos de habilidades digitales debe ser accesible, oportuna y de calidad el Centro de Investigación Estadística ERGOSTATS S.A.S ha decidido emprender el programa “Nido del Buho”, una iniciativa que involucra a estudiantes, profesores y al personal de la academia para fomentar la educación desde una perspectiva articulada, solidaria y participativa.\n“Nido del Buho” busca vincular los esfuerzos de personal profesional con experiencia en el análisis estadístico de datos para contribuir a la reducción de las brechas entre tres elementos: las necesidades de educación complementaria de los estudiantes, las mallas curriculares que tienen componentes de programación y análisis de datos, y las necesidades de las empresas que confirman los distintos sectores económicos del país.\nDentro del componente 1 del proyecto (Planificación), se requiere el desarrollo de instrumentos y materiales de enseñanza que permita cubrir de manera efectiva el diseño curricular alineado a la malla curricular propuesta por las organizaciones estudiantiles. Mismo que deberá cumplir con los estándares, lineamientos, directrices, metodologías y manuales que ERGOSTATS ha diseñado para los cursos de formación y capacitación.\nPor otro lado, el componente 3 del proyecto (Ejecución) establece que la puesta en escena de los cursos debe estar determinada por las metodologías de enseñanza de ERGOSTATS respetando el diseño y estructura de clases adaptado al material desarrollado para cada caso específico.\nAmbos componentes son competencia del personal académico y administrativo de ERGOSTATS en conjunto con las organizaciones estudiantes. Para ello se ha llegado a la alianza académica con la Asociación de Estudiantes de la Carrera de Economía Cuantitativa de la Escuela Politécnica Nacional para la ejecución del curso “Introducción al lenguaje de programación Python para el análisis estadístico” en el contexto del programa “Nido del Búho”"
  },
  {
    "objectID": "Clase_1.html",
    "href": "Clase_1.html",
    "title": "Introducción a Python y su entorno de desarrollo",
    "section": "",
    "text": "Resumen\nEste capítulo introducimos Python como un lenguaje de programación orientado a objetos, destacando sus características clave como su sintaxis, gramática, entre otros. Exploramos la historia de Python, destacando la visión de Guido van Rossum de crear un lenguaje legible y fácil de mantener. Se subraya el carácter de código abierto de Python, resaltando su desarrollo y mantenimiento por una comunidad global de voluntarios como tu y como yo.\nDespues de este capítulo realizar la instalación de módulos o paquetes comunes en Python, como Pandas, NumPy, SciPy y Matplotlib, lo que hará más sencillo tu trabajo estadístico. Asi mismo, la sección sobre la búsqueda de ayuda con Python destaca recursos útiles como la documentación oficial, Stack Overflow, Real Python, Python Tutor y el uso de la función help(). Finalmente, se presentan buenas prácticas para programar en Python, especialmente enfocadas en aplicaciones estadísticas, y se proporciona una guía para instalar y configurar Python y Visual Studio Code, preparando al lector para un desarrollo efectivo y eficiente.\nPython es un lenguaje de programación orientado a objetos porque proporciona características que soportan la programación orientada a objetos, que incluye la definición de clases, la herencia y la encapsulación. En Python, todo es un objeto, incluyendo los números, las cadenas y las funciones.\nEs muy particular para cada lenguaje:\nEl orden importa como en este ejemplo:\nDe la misma manera:\n# Ejecución correcta:\nx = 5\nprint(x)\n\ndel x\n# Ejecución incorrecta:\nprint(x)\nx = 5\npip es el sistema de gestión de paquetes de Python, que te permite instalar y administrar paquetes de software adicionales que no se incluyen en la biblioteca estándar de Python.\nAquí te mostramos cómo puedes instalar algunos paquetes comunes. Abre tu terminal o línea de comandos e introduce los siguientes comandos:\nRecuerda que debes tener instalado Python y pip en tu sistema para poder ejecutar estos comandos. Si estás utilizando un entorno virtual (lo cual es una buena práctica), estos paquetes se instalarán en el entorno virtual en lugar de en tu instalación global de Python.\nPara acceder a la documentación oficial de Python, puedes visitar el siguiente enlace: Documentación oficial de Python. Aquí encontrarás guías, tutoriales y referencias detalladas sobre el lenguaje y su biblioteca estándar.\nAdemás de la documentación oficial, aquí tienes tres fuentes adicionales de consulta para obtener ayuda con Python:\nPor ejemplo, durante el curso vamos a trabajar con Visual Studio Code, si tienes el siguiente código:\nimport numpy as np\n\nnp.array()\nCuando coloques el cursor sobre array o después de abrir el paréntesis, Visual Studio Code mostrará un tooltip con la ayuda de la función np.array.\nTambién puedes usar la función incorporada help() en la terminal de Python para obtener ayuda sobre una función. Por ejemplo:\nhelp(np.array)\nEsto imprimirá la documentación de la función np.array en la terminal.\nConsiserando que vamos a hacer aplicaciones estadísticas:\nTe dejamos una guía paso a paso para instalar Visual Studio Code y Python:\nCon estos pasos, deberías estar listo para empezar a programar en Python usando Visual Studio Code."
  },
  {
    "objectID": "Clase_1.html#cuál-fue-la-motivación-para-crear-python",
    "href": "Clase_1.html#cuál-fue-la-motivación-para-crear-python",
    "title": "Introducción a Python y su entorno de desarrollo",
    "section": "¿Cuál fue la motivación para crear Python?",
    "text": "¿Cuál fue la motivación para crear Python?\nPython fue creado por Guido van Rossum en 1989. La motivación principal era crear un lenguaje de alto nivel que fuera fácil de leer y de escribir. Guido quería que Python fuera un lenguaje que pudiera hacer las cosas de manera rápida, pero que también permitiera escribir código que fuera fácil de entender y de mantener. Guido trabajó en el proyecto durante su tiempo en el Centro para las Matemáticas y la Informática (CWI) en los Países Bajos."
  },
  {
    "objectID": "Clase_1.html#pyhton-un-lenguaje-open-source",
    "href": "Clase_1.html#pyhton-un-lenguaje-open-source",
    "title": "Introducción a Python y su entorno de desarrollo",
    "section": "Pyhton, un lenguaje open source",
    "text": "Pyhton, un lenguaje open source\nPython es un lenguaje de programación de código abierto. Esto significa que su código fuente es libremente disponible y puede ser distribuido y modificado. Python es desarrollado y mantenido por una comunidad de voluntarios de todo el mundo que colaboran a través de la Python Software Foundation.\n\n\nPuedes acceder a la Python Software Foundation en este enlace Python Software Foundation"
  },
  {
    "objectID": "Clase_2.html",
    "href": "Clase_2.html",
    "title": "Introducción al manejo de datos en Python con pandas",
    "section": "",
    "text": "Resumen\nEn la era digital actual, la capacidad de analizar y manejar datos se ha vuelto esencial en una variedad de disciplinas, incluida la economía. Python, un lenguaje de programación versátil y poderoso, se ha convertido en una herramienta fundamental para el análisis de datos.\nEn este capítulo, exploraremos cómo Python, en combinación con la biblioteca Pandas, puede potenciar tus habilidades en el manejo eficiente y efectivo de datos económicos.\nEn el corazón del análisis de datos con Python se encuentran las variables y los tipos de datos. Python ofrece una variedad de tipos estándar, incluyendo números, cadenas, listas, tuplas y diccionarios. En este subtema, exploraremos cómo declarar variables, comprender la dinámica de los tipos de datos, y aprovechar las estructuras fundamentales de almacenamiento de información. Al dominar el manejo de variables y tipos de datos, los analistas pueden organizar y manipular datos de manera eficaz, allanando el camino para un análisis preciso y significativo.\nVamos a ver cuales son los tipos de objetos más frecuentes en Python:\n# Esto es una variable de texto\nmi_variable = \"Hola Mundo\"\n\n# Esto es una lista de números\nmi_lista = [1, 2, 3, 4, 5]\n\n# Esto es un diccionario \nmi_diccionario = {\"clave\": \"valor\", \"clave_2\": \"valor_2\"}\nMientras que ahora te presentamos los tipos de variable que\nUsos:\n# Creemos vectores con 5 elementos repetidos cada uno\nvector_entero = [10] * 5\nvector_flotante = [3.14] * 5 # Con decimales\nvector_complejo = [(1 + 2j)] * 5\n\n# Crear un diccionario que contenga estos vectores\ndiccionario = {\n    \"entero\": vector_entero,\n    \"flotante\": vector_flotante,\n    \"complejo\": vector_complejo\n}\n\nprint(diccionario)\nUsos:\ncadena_simple = 'Hola, mundo!'\n\ncadena_doble = [\"¡Python es poderoso!\", \"Me gusta aprender\"]\nvalores_logicos = [True, False]\nUsos:\nLa información proviene de diversas fuentes: hojas de cálculo, bases de datos, archivos CSV, y más. En este subcapítulo, aprenderemos cómo Python, junto con la biblioteca Pandas, puede funcionar como un puente eficaz para importar y manipular datos de diferentes fuentes. Este proceso es fundamental para transformar datos crudos en conocimientos significativos.\nUtilizaremos las tablas llamadas IMP_SRI.xlsx, ventas_SRI.CSV, riesgo_pais.txt\nEn el análisis de datos con Python, las estructuras de control como ‘if’, ‘for’, y ‘while’ desempeñan un papel clave en la manipulación y procesamiento eficiente y ordenado de datos.\nLa estructura condicional ‘if’ permite tomar decisiones basadas en condiciones específicas, mientras que los bucles ‘for’ y ‘while’ posibilitan recorrer conjuntos de datos y realizar iteraciones, respectivamente. Estas herramientas permiten a los analistas personalizar el flujo de ejecución de sus programas, optimizando la manipulación de datos y simplificando tareas repetitivas en el análisis de datos.\nPrimero veamos algunos ejemplos sencillos:\n# Estructura if\nif mi_variable == \"Hola Mundo\":\n    print(\"¡Hola Mundo!\")\n\n# Estructura for\nfor i in mi_lista:\n    print(i)\n\n# Estructura while\ncontador = 0\nwhile contador &lt; 5:\n    print(contador)\n    contador += 1\nEn Python, el bucle for se utiliza para iterar sobre una secuencia (como una lista, tupla, diccionario, conjunto o cadena) o cualquier otro objeto iterable.\nVeamos un ejemplo, si tienes un vector adicional llamado impuestos y quieres calcular el ratio de utilidad después de impuestos a costo, puedes hacerlo de la siguiente manera:\nutilidad = [10, 20, 30, 40, 50]\ncosto = [20, 20, 30, 40, 40]\nimpuestos = [2, 4, 6, 8, 10]\n\nfor u, c, i in zip(utilidad, costo, impuestos):\n    utilidad_despues_impuestos = u - i\n    ratio = utilidad_despues_impuestos / c\n    if ratio &lt; 1:\n        print(f\"Deficit: La utilidad después de impuestos {utilidad_despues_impuestos} es menor que el costo {c}.\")\n    elif ratio == 1:\n        print(f\"Equilibrio: La utilidad después de impuestos {utilidad_despues_impuestos} es igual al costo {c}.\")\n    else:\n        print(f\"Ganancia: La utilidad después de impuestos {utilidad_despues_impuestos} es mayor que el costo {c}.\")\nLas funciones en Python representan bloques de código reutilizables que promueven la modularidad y organización en el análisis de datos. En este subtema, exploraremos la creación y aplicación de funciones, permitiéndonos encapsular tareas específicas para su fácil reutilización. Aprenderemos a definir funciones con parámetros, gestionar valores de retorno y entender el ámbito de las variables. Al incorporar funciones en nuestro flujo de trabajo, no solo simplificamos el código, sino que también mejoramos la mantenibilidad y legibilidad, optimizando así el proceso de análisis de datos con Python.\n# Definición de una función\ndef mi_funcion():\n    print(\"¡Hola desde mi función!\")\n\n# Llamada a la función\nmi_funcion()"
  },
  {
    "objectID": "Clase_2.html#librería-pandas",
    "href": "Clase_2.html#librería-pandas",
    "title": "Introducción al manejo de datos en Python con pandas",
    "section": "Librería Pandas",
    "text": "Librería Pandas\nPandas, una biblioteca de Python diseñada específicamente para el análisis y manejos de datos, ofrece estructuras de datos flexibles y herramientas de manipulación que facilitan la tarea de explorar y comprender conjuntos de datos complejos.\nEn este curso, nos sumergiremos en el mundo de Pandas para descubrir cómo puede facilitar el análisis y la manipulación de datos económicos."
  },
  {
    "objectID": "Clase_2.html#que-es-un-dataframe",
    "href": "Clase_2.html#que-es-un-dataframe",
    "title": "Introducción al manejo de datos en Python con pandas",
    "section": "¿ Que es un DataFrame?",
    "text": "¿ Que es un DataFrame?\nUn DataFrame es una estructura de datos con dos dimensiones en la cual se puede guardar datos de distintos tipos (como caractéres, enteros, valores de punto flotante, factores y más) en columnas. Es similar a una hoja de cálculo o una tabla de SQL o el data.frame de R. Un DataFrame siempre tiene un índice (con inicio en 0). El índice refiere a la posición de un elemento en la estructura de datos. Primero vamos a importar pandas.\n\nimport pandas as pd\n\n\nEjemplo: Imaginemos que tenemos información sobre el rendimiento de nuestros amigos en diferentes juegos durante una semana. Queremos organizar estos datos de manera clara y fácil de entender, ¡y para eso usaremos Pandas!\n\n\n# Crear un DataFrame con los datos de rendimiento en juegos\ndatos = {\n    'Nombre': ['Juan', 'María', 'Carlos', 'Ana'],\n    'Juego 1 (puntos)': [150, 180, 130, 200],\n    'Juego 2 (puntos)': [120, 90, 110, 150],\n    'Juego 3 (puntos)': [200, 160, 180, 190]\n}\n\ndf = pd.DataFrame(datos)\n\n# Mostrar el DataFrame\nprint(df)\n\n\n\n¡Con Pandas, explorar y entender datos se vuelve tan fácil como jugar un juego!\n\nEn este ejemplo:\n\nLa columna Nombre contiene los nombres de nuestros amigos.\nLas columnas Juego 1 (puntos), Juego 2 (puntos), y Juego 3 (puntos) representan los puntos obtenidos por cada amigo en diferentes juegos.\n\nEl DataFrame nos permite organizar estos datos de manera clara. Ahora, podemos realizar diversas operaciones para entender mejor el rendimiento de cada amigo, como calcular promedios, encontrar el máximo puntaje, o incluso visualizar los datos de manera gráfica."
  },
  {
    "objectID": "Clase_2.html#importación-de-un-archivo-.xlsx",
    "href": "Clase_2.html#importación-de-un-archivo-.xlsx",
    "title": "Introducción al manejo de datos en Python con pandas",
    "section": "Importación de un archivo .xlsx",
    "text": "Importación de un archivo .xlsx\nEn la ciencia de datos, es común trabajar con conjuntos de datos almacenados en diversos formatos, y uno de los formatos populares para el almacenamiento de datos tabulares es el formato Excel (.xlsx). Pandas, una biblioteca de Python ampliamente utilizada para el análisis de datos, proporciona una funcionalidad sencilla y poderosa para importar datos desde archivos .xlsx.\nExploraremos cómo utilizar la biblioteca pandas para importar datos desde un archivo Excel (.xlsx) y cargarlos en un DataFrame.\n\nimp_sri = pd.read_excel (\"data/otras_fuentes/IMP_SRI.xlsx\")\nprint(df)\n\nAhora revisaremos los tipos de datos que contiene nuestro DataFrame con dataframe.dtypes\n\nint64 : para enteros de 64 bits\nfloat64: para números de punto flotante de 64 bits\nobject : para objetos de texto (cadenas o mixtos)\n\n\nObserva como en estas lineas de código revisamos los atributos de nuestro Pandas Dataframe, estos atributos nos dan idea de la estructura del dataframe.\n\n\nimp_sri.dtypes\n\nAhora revisamos el numbre de las variables (columnas)\n\nimp_sri.columns\n\nSi deseamos saber cuantas filas y columnas tiene nuestro DataFrame (df), lo hacemos con dataframe.shape, esto nos muestra que tenemos 18 observaciones (observaciones) con 5 variables (columnas)\n\nimp_sri.shape\n\n\n\nAsi mismo observa que algunos un Pandas Dataframe tiene atributos y métodos. En este caso describe() y head() son métodos que requieren o no argumentos. Siempre puedes usar el comando print(dir(df)) para revisar los métodos y atributos. Asi mismo no olvides que puedes usar help(df.describe) para recibir ayuda de Python.\nSi deseamos realizar una estadistica descriptiva rapida podemos utilizar dataframe.describe()\n\nimp_sri.describe()\n\nAhora si deseamos unicamente imprimir las 10 primeras observaciones lo realizamos de la siguiente manera utilizando dataframe.head(n) siendo n el número de observaciones que deseamos observar, en este caso 10.\n\nimp_sri.head(10)"
  },
  {
    "objectID": "Clase_2.html#importación-de-un-archivo-.csv",
    "href": "Clase_2.html#importación-de-un-archivo-.csv",
    "title": "Introducción al manejo de datos en Python con pandas",
    "section": "Importación de un archivo .CSV",
    "text": "Importación de un archivo .CSV\nLa importación de datos desde archivos en formato CSV (Comma-Separated Values) es una tarea común en el análisis de datos. Los archivos CSV son ampliamente utilizados debido a su simplicidad y compatibilidad con una variedad de aplicaciones. Pandas facilita la importación de datos CSV mediante la función read_csv().\nExploraremos cómo cargar datos desde un archivo CSV en un DataFrame utilizando pandas. Veremos cómo manejar archivos CSV que contienen información tabular y cómo aprovechar las capacidades de pandas para realizar operaciones sobre estos datos.\nImportaremos la tabla “ventas_SRI.csv”, la cual presenta un registro detallado de las ventas efectuadas en Ecuador por diversas industrias desde el 2006. Esta tabla distingue entre las ventas sujetas a impuestos del IVA al 12% y aquellas que están exentas de este impuesto, gravadas al 0%.\n\nventas_sri = pd.read_csv (\"data/otras_fuentes/ventas_SRI.csv\", delimiter = ';')\nprint(ventas_sri)\n\nAhora repliquemos lo que ya ralizamos anteriormente:\n\nventas_sri.dtypes\nventas_sri.columns\nventas_sri.describe()\nventas_sri.head (10)"
  },
  {
    "objectID": "Clase_2.html#importación-de-un-.txt",
    "href": "Clase_2.html#importación-de-un-.txt",
    "title": "Introducción al manejo de datos en Python con pandas",
    "section": "Importación de un .txt",
    "text": "Importación de un .txt\nEn el ámbito económico, la importación efectiva de datos es esencial para desentrañar patrones y tendencias clave. En este subsubcapítulo, exploraremos cómo utilizar pandas en Python para importar datos desde archivos de texto (.txt), una tarea crucial para analistas económicos y profesionales del sector. Desde indicadores económicos hasta tasas de cambio y riesgo país, aprenderemos a convertir estos datos textuales en estructuras tabulares (DataFrames). Abordaremos desafíos específicos, como la elección de codificaciones y la gestión de delimitadores, proporcionando habilidades prácticas para aprovechar al máximo la información contenida en archivos de texto y potenciar el análisis económico con datos fiables.\nImportaremos el documento riesgo_pais_1.txt donde describe el riesgo país del Ecuador desde el 2004.\n\nriesgo_pais = pd.read_table('data/otras_fuentes/riesgo_pais_1.txt', delimiter='\\t')\n# El parámetro encoding='latin-1' que se proporciona al leer un archivo con pandas especifica la codificación de caracteres que se utilizará para decodificar el contenido del archivo de texto.\n\nprint(riesgo_pais)"
  },
  {
    "objectID": "Clase_2.html#if-el-operador-condicional",
    "href": "Clase_2.html#if-el-operador-condicional",
    "title": "Introducción al manejo de datos en Python con pandas",
    "section": "if el operador condicional",
    "text": "if el operador condicional\nEl comando if en Python no funciona directamente con vectores como lo haría en lenguajes como R o MATLAB. En Python, tendrías que iterar sobre los elementos del vector y aplicar la condición if a cada elemento.\nPor ejemplo, si tienes dos listas, utilidad y costo, y quieres verificar si la relación utilidad/costo es mayor o menor que 1 para cada elemento, podrías hacer algo como esto:\n\nutilidad = [10, 20, 30, 40, 50]\ncosto = [20, 20, 30, 40, 40]\n\nfor u, c in zip(utilidad, costo):\n    ratio = u / c\n    if ratio &lt; 1:\n        print(f\"Deficit: La utilidad {u} es menor que el costo {c}.\")\n    elif ratio == 1:\n        print(f\"Equilibrio: La utilidad {u} es igual al costo {c}.\")\n    else:\n        print(f\"Ganancia: La utilidad {u} es mayor que el costo {c}.\")\n\n\n\nEn este código, zip(utilidad, costo) empareja cada elemento de utilidad con el correspondiente elemento de costo. Luego, para cada par de valores, calculamos el ratio u / c y usamos una estructura if para imprimir un mensaje basado en si el ratio es menor que, igual a, o mayor que 1.\nAlgunas consideraciones importantes al usar if en Python:\n\nPython usa indentación para delimitar bloques de código. Asegúrate de indentar correctamente tu código dentro de la estructura if.\nPython usa == para comparar igualdad, != para desigualdad, &lt; para menor que, &gt; para mayor que, &lt;= para menor o igual que, y &gt;= para mayor o igual que.\nPuedes usar and y or para combinar condiciones, y not para negar una condición.\nLa estructura if puede ser seguida por una o más estructuras elif (abreviatura de “else if”) para comprobar múltiples condiciones, y una estructura else para especificar qué hacer si ninguna de las condiciones anteriores es verdadera."
  },
  {
    "objectID": "clase_3.html",
    "href": "clase_3.html",
    "title": "Funciones y la importancia del análisis de datos",
    "section": "",
    "text": "Resumen\nEn este capitulo 3, exploramos conceptos clave para un análisis efectivo de datos y cómo Python, especialmente con módulos como Pandas, se utiliza en iniciativas para abordar temas sociales en la investigación académica. Comenzamos definiendo el análisis de datos como un proceso integral para descubrir información útil, apoyar decisiones y comprender tanto datos numéricos como no numéricos. El proceso implica definir objetivos, seleccionar métricas, recolectar y organizar datos, clasificar y analizar estos datos, e interpretar los resultados, siempre conscientes de posibles sesgos y limitaciones.\nDiferenciamos entre análisis cualitativo y cuantitativo, destacando la importancia de ambos en distintos contextos. Examinamos herramientas clave para el análisis de datos, incluyendo Python, R, Power BI, Tableau, Excel, SQL y Qlik, cada una adecuada para diferentes necesidades y habilidades.\nA través de ejemplos prácticos y casos de estudio, demostramos cómo el análisis de datos se aplica en la vida real, incluyendo ejemplos en sectores como la banca y la moda. Discutimos el papel de Python en la sociedad, abarcando desde el análisis de datos hasta la investigación social y académica, y presentamos historias de éxito que ilustran su aplicación práctica.\nFinalmente, nos centramos en las funciones en Python, destacando su importancia en la reutilización de código y eficiencia. Cubrimos buenas prácticas en la escritura de funciones, como nombres descriptivos, uso de docstrings, y la importancia de funciones dedicadas a una sola tarea. Con ejemplos prácticos, mostramos cómo implementar estas prácticas para funciones específicas, incluyendo una función para identificar números primos, demostrando así la utilidad y flexibilidad de Python en el análisis de datos y programación general.\nLa pergunta que todos se hacen es: ¿sirve Python en el mundo real? La respuesta es sí, muchos avances se han logrado con este lenguaje, es por ello que hemos puesto aquí algunos de ellos.\nMantener un trabajo de análisis de datos ordenado y reproducible es fundamental para la integridad y eficacia de cualquier proyecto. Python, en conjunto con herramientas como Visual Studio Code, facilita esta tarea. La reproducibilidad no solo es crucial para validar resultados, sino también para compartir hallazgos y colaborar con otros. Al trabajar con Python en un entorno como Visual Studio Code, se beneficia de:\nEn resumen, Python y Visual Studio Code forman un dúo poderoso para el análisis de datos, ofreciendo un entorno eficiente, reproducible y colaborativo. La combinación de la simplicidad y potencia de Python con las avanzadas características de Visual Studio Code empodera a los analistas y científicos de datos para abordar proyectos complejos con mayor eficacia y transparencia.\nCuando se trabaja con conjuntos de datos en Python, especialmente con DataFrames de Pandas, una tarea común es realizar análisis estadísticos agrupados por ciertas categorías. En esta sección, nos centraremos en un DataFrame que incluye variables como ventas, número de empleados, compras e impuestos. Vamos a explorar cómo se puede utilizar Python para calcular estadísticas clave como la media, la mediana, y los percentiles 25 y 75, agrupando los datos según el sexo del dueño de los establecimientos en Ecuador."
  },
  {
    "objectID": "clase_3.html#qué-es-el-análisis-de-datos",
    "href": "clase_3.html#qué-es-el-análisis-de-datos",
    "title": "Funciones y la importancia del análisis de datos",
    "section": "¿Qué es el análisis de datos?",
    "text": "¿Qué es el análisis de datos?\nEl análisis de datos es un proceso integral y multifacético que implica examinar, limpiar, transformar y modelar datos con el objetivo de descubrir información útil, llegar a conclusiones y apoyar la toma de decisiones. Este proceso está en el corazón de la investigación académica, las estrategias de mercadotecnia, la gestión de recursos humanos, y se extiende hasta el dominio de la política pública y más allá. Las técnicas estadísticas se aplican para descubrir patrones, identificar relaciones y tendencias, y probar hipótesis. ::: column-margin\nNota: Aunque el análisis de datos puede parecer un campo dominado por números y estadísticas, su alcance va más allá, abarcando interpretaciones cualitativas de textos, imágenes y otros medios no estructurados.\n::: ## ¿Cómo se realiza el análisis de datos?\nEl proceso de análisis de datos puede variar en función del contexto y los objetivos específicos, pero generalmente sigue una estructura lógica y metódica que incluye:\n\nDefinición de objetivos: Determinar el propósito del análisis. ¿Qué preguntas específicas estamos tratando de responder? Es esencial establecer metas claras y medibles.\nSelección de métricas: Identificar qué datos serán recogidos y cómo. Por ejemplo, al medir los productos más vendidos, considerar aspectos como los canales de marketing, las plataformas de compra y las preferencias del cliente.\nRecolección y organización de datos: Adquirir los datos necesarios de fuentes confiables y organizarlos de manera que sean accesibles y manejables para el análisis.\nClasificación y análisis de datos: Utilizar métodos estadísticos y de minería de datos para examinar y modelar los datos.\nInterpretación de resultados: Extraer significado de los datos analizados y comunicar los hallazgos de manera efectiva.\n\n\nSugerencia: Al interpretar los resultados, es crucial ser consciente de los posibles sesgos y limitaciones de los datos y métodos utilizados."
  },
  {
    "objectID": "clase_3.html#tipos-de-análisis-de-datos",
    "href": "clase_3.html#tipos-de-análisis-de-datos",
    "title": "Funciones y la importancia del análisis de datos",
    "section": "Tipos de análisis de datos",
    "text": "Tipos de análisis de datos\nEl tipo de análisis de datos a realizar depende de la naturaleza de la información que se maneja:\n\nAnálisis cualitativo: Se centra en datos no numéricos como textos, entrevistas, imágenes y observaciones. Responde preguntas como “¿Cómo?” y “¿Por qué?” y se preocupa por el contexto y el significado.\nAnálisis cuantitativo: Trata con datos numéricos y cuantificables, a menudo utilizando estadísticas y modelos matemáticos para probar hipótesis y predecir tendencias."
  },
  {
    "objectID": "clase_3.html#herramientas-para-el-análisis-de-datos",
    "href": "clase_3.html#herramientas-para-el-análisis-de-datos",
    "title": "Funciones y la importancia del análisis de datos",
    "section": "Herramientas para el análisis de datos",
    "text": "Herramientas para el análisis de datos\nEl análisis de datos se beneficia enormemente de la variedad de herramientas disponibles, cada una con sus fortalezas:\n\nPower BI: Una herramienta de visualización de datos y business intelligence de Microsoft.\nR: Un lenguaje de programación orientado a la estadística y el análisis de datos.\nPython: Un lenguaje de programación de alto nivel con potentes bibliotecas para análisis de datos como Pandas y NumPy.\nTableau: Una herramienta de visualización de datos intuitiva para crear dashboards interactivos.\nExcel: Una aplicación de hoja de cálculo versátil y ampliamente utilizada para análisis básicos.\nSQL: Un lenguaje de consulta estructurado para la gestión de bases de datos.\nQlik: Una plataforma de análisis y visualización de datos orientada al usuario de negocios.\n\nCada una de estas herramientas tiene su propia curva de aprendizaje y se adapta mejor a diferentes necesidades y habilidades. Por ejemplo, R y Python requieren una base en programación, mientras que herramientas como Power BI y Tableau son más accesibles para los no programadores.\n\nEjemplo práctico: Para ilustrar cómo estas herramientas pueden ser utilizadas en la vida real, considere un caso de estudio en el que Python se utiliza para analizar tendencias de ventas y luego Tableau para visualizar\n\n\n\n\n!Siempre es importante gestionar el trabajo en equipo cuando empleamos varias herramientas en un mismo trabajo¡"
  },
  {
    "objectID": "clase_3.html#análisis-de-datos-en-la-vida-real",
    "href": "clase_3.html#análisis-de-datos-en-la-vida-real",
    "title": "Funciones y la importancia del análisis de datos",
    "section": "Análisis de datos en la vida real",
    "text": "Análisis de datos en la vida real\nExisten varios ejemplos de cómo se ha aplicado el análisis de datos en el día a día, aquí mostramos algunos de ellos.\n\nBBVA y la Navidad\nResumiendo, se deseaba determinar en qué gastaban más su dinero los clientes en la época navideña, pues cómo sabemos, es una época en la que los gastos suben. Se puede leer esta investigación en su página siguiendo el siguiente enlace: https://www.bbva.com/es/navidata-gastado-dinero-esta-navidad/\nInditex y videojuegos\nLa empresa busco a una compañia de videojuegos para que les ayuden en el análisis de datos para así no quedarse atrás. Se puede leer el artículo completo aquí: https://www.lavozdegalicia.es/noticia/galicia-economica/2020/03/01/inditex-ficha-experta-videojuegos-explotar-mina-oro-big-data/00031583073737230198202.htm\n\nExisten muchas más historias relacionadas al análisis de datos, aunque no es fácil llegar a ser considerado un “analista de datos”, con dedicación se puede lograr mucho en este campo que esta emergiendo rápidamente."
  },
  {
    "objectID": "clase_3.html#python-y-sus-usos-en-la-sociedad",
    "href": "clase_3.html#python-y-sus-usos-en-la-sociedad",
    "title": "Funciones y la importancia del análisis de datos",
    "section": "Python y sus usos en la sociedad",
    "text": "Python y sus usos en la sociedad\nComo sabemos Python es un lenguaje muy usado en la actualidad, el mismo puede ser aplicado a muchos campos. Si entramos a temas sociales Python puede ser usado en:\n\nAnálisis de datos\nInvestigación social y académica\nVisualizar datos\nProcesamiento de lenguaje natural\nDesarrollo de aplicaciones\nAnálisis en redes sociales\n\nPor lo que Python es una herramienta escencial al momento de querer realizar análisis de datos, nos facilita la comprensión y nos da una forma de abordar los poblemas."
  },
  {
    "objectID": "clase_3.html#caso-de-gusto.com",
    "href": "clase_3.html#caso-de-gusto.com",
    "title": "Funciones y la importancia del análisis de datos",
    "section": "Caso de Gusto.com",
    "text": "Caso de Gusto.com\nEmpezó como un sitio de viajes en línea llamado EZTrip.com, evolucionó para ofrecer servicios de reserva y también se diversificó con contenido de reseñas de viajes, blogs y fotos. La compañía utiliza sistemas de reserva en línea y se enfrenta a desafíos al integrar sistemas antiguos basados en mainframes y sistemas más modernos mediante técnicas como “screen scraping”. Han adoptado Python para facilitar la integración y mejorar la eficiencia en tareas diarias.\n\n¿Por qué Python?\nLos sistemas de reserva basados en mainframes presentan limitaciones en la entrada de datos y formatos, lo que hace que el uso de Python sea crucial en tareas como procesamiento de texto, limpieza de datos y desarrollo rápido. Python ha sido fundamental en la construcción de una red social para viajeros y en la transición hacia un modelo de contenido generado por usuarios.\nGusto.com utiliza Python para procesar grandes conjuntos de datos de manera eficiente, como en la creación de un sistema de localización para descripciones de hoteles. Python también ha sido efectivo en la integración de servicios web de proveedores, y se ha destacado por su capacidad de desarrollo rápido y sus herramientas de procesamiento de XML.\nLa elección de Python se basó en su portabilidad, acceso al código fuente, capacidad de integración, desarrollo rápido, soporte para optimización selectiva, scriptabilidad y facilidad de aprendizaje. Además, la comunidad de Python y la independencia del sistema operativo han sido aspectos beneficiosos para Gusto.com. Python ha demostrado ser una herramienta clave en el éxito y crecimiento de Gusto.com en el espacio de viajes en línea.\nPuedes leer el artículo completo en: https://www.python.org/about/success/gusto/"
  },
  {
    "objectID": "clase_3.html#strakt-y-python",
    "href": "clase_3.html#strakt-y-python",
    "title": "Funciones y la importancia del análisis de datos",
    "section": "Strakt y Python",
    "text": "Strakt y Python\nStrakt, ha tenido éxito con el proyecto CAPS (Collaborative Approach to Problem Solving). Este proyecto, ofrece soluciones modulares de colaboración y flujos de trabajo, utilizando Python como lenguaje principal.\n\n¿ Por qué Python?\nPython fue elegido por su portabilidad, acceso al código fuente, capacidad de integración con sistemas existentes, desarrollo rápido, soporte para optimización selectiva, scriptabilidad y por ser fácil de aprender. A pesar de ser nuevos en Python, los desarrolladores de Strakt encontraron que el lenguaje era fácil de aprender y aumentaba la productividad. Python también influyó positivamente en el proceso de diseño y facilitó las revisiones de código.\nEl proyecto CAPS fue exitoso, y la implementación se desplegó en una universidad sueca como cliente beta. La elección de Python ha sido fundamental para el éxito de Strakt en el desarrollo de soluciones colaborativas y de flujo de trabajo.\nSe puede encontrar el resumen completo en: https://www.python.org/about/success/strakt/\nComo podemos observar el uso de Python es diverso, pero ahora entremos en el uso de la librería pandas. Veamos esta divertida forma de aprender su uso\n\n\nHistorias de éxito como las de Gusto.com y Strakt ilustran cómo Python, con su amplia gama de bibliotecas y comunidad activa, es fundamental en la integración de sistemas complejos y en el procesamiento eficiente de grandes conjuntos de datos. La capacidad de Python para trabajar con texto, números, imágenes y datos complejos lo convierte en una herramienta versátil y potente para el análisis de datos."
  },
  {
    "objectID": "clase_3.html#pandas-y-pokémon",
    "href": "clase_3.html#pandas-y-pokémon",
    "title": "Funciones y la importancia del análisis de datos",
    "section": "Pandas y Pokémon",
    "text": "Pandas y Pokémon\nPara todos los interesados en aprender más sobre análisis de datos utilizando Python, especialmente con la biblioteca Pandas, hay una oportunidad emocionante y única que no deben perderse. Les invito a explorar el enlace: Aprende Pandas con Pokémon. Esta guía combina el aprendizaje práctico de Pandas, una herramienta esencial en el análisis de datos con Python, con el atractivo y la familiaridad del mundo de Pokémon.\nEs una forma excelente y divertida de entender los conceptos fundamentales de Pandas, aplicándolos a un conjunto de datos interesante y amigable. Ideal tanto para principiantes en Python como para aquellos que buscan solidificar sus habilidades en el manejo de datos, este recurso ofrece una experiencia de aprendizaje interactiva y atractiva. ¡No dejen pasar la oportunidad de mejorar sus habilidades en análisis de datos con este recurso único!\nEste divertido post lo puedes encontrar en: https://www.kaggle.com/code/ash316/learn-pandas-with-pokemons\nEl mismo ayuda a entender las bases del análisis de datos de una manera más amigable.\n\nEl análisis de datos ha revolucionado innumerables campos, desde la academia hasta la industria, ofreciendo insights profundos y conduciendo a decisiones informadas. Python, como herramienta open source y lenguaje amigable, ha jugado un papel crucial en esta revolución. Su simplicidad y poder han permitido a analistas y científicos de datos de todos los niveles, desde principiantes hasta expertos, procesar, analizar y visualizar datos de manera efectiva."
  },
  {
    "objectID": "clase_4.html",
    "href": "clase_4.html",
    "title": "Variables aleatorias, funciones de distribución y estimación de parametros",
    "section": "",
    "text": "Resumen\nEn el fascinante mundo de la estadística y la programación, las variables aleatorias juegan un papel crucial. Como estudiantes que están explorando el poder de Python en el análisis estadístico, es esencial comprender estos conceptos. Las variables aleatorias no solo nos permiten modelar y analizar fenómenos aleatorios del mundo real, sino que también son la base para avanzar en técnicas más complejas de análisis de datos. En esta guía, abordaremos desde lo más básico hasta conceptos más avanzados, siempre con ejemplos prácticos en Python para una mejor comprensión. Así que, ¡prepárense para sumergirse en el mundo de las variables aleatorias y su aplicación en Python!"
  },
  {
    "objectID": "clase_4.html#variables-aleatorias",
    "href": "clase_4.html#variables-aleatorias",
    "title": "Variables aleatorias, funciones de distribución y estimación de parametros",
    "section": "Variables Aleatorias",
    "text": "Variables Aleatorias\n\n\nCon una variable aleatoria no sabemos con certeza qué valor tomará hasta que se realiza el experimento o se observa el fenómeno. En el idioma inglés es común decir que el valor de la variable se ha realizado cuando una variable aleatoria toma un valor y se convierte en una observación o dato.\n\nUna variable aleatoria puede ser vista como un puente entre el mundo real y el mundo matemático. En términos simples, es una variable que toma valores numéricos determinados por el resultado de un fenómeno aleatorio. A cada uno de estos resultados se lo asocia con una probabilidad como veremos más adelante. A diferencia de una variable determinística, cuyo valor es constante o predecible, una variable aleatoria tiene un grado de incertidumbre.\nPensemos en algunos ejemplos de variable aleatoria en la vida cotidiana:\n\nEl número de pichirilos que podemos encontrar cada 20 carros que veamos con una tasa de exito aproximada del 1%.\nEl número de estudiantes atendidos por la secretaria del departamento de ciencias cada 5 minutos.\nLa suma de dos dados lanzados.\nLos salarios de los trabajadores de una empresa en el mes de diciembre 2024.\n\nFijate que en estos fenómenos de ejemplo, hemos definido unas condiciones específicas para reproducir los resultados y compararlos entre distintas replicas u observaciones.\n\nFunciones de Distribución\nUna función de distribución describe cómo se distribuyen los valores de una variable aleatoria. Es como un mapa que nos muestra la probabilidad de que la variable aleatoria tome un valor específico o caiga dentro de un rango determinado. Esta función es fundamental para entender el comportamiento de variables aleatorias.\nExisten varios tipos, cada uno modelando diferentes situaciones. Por ejemplo, la distribución binomial modela escenarios con resultados de ‘éxito’ o ‘fracaso’, mientras que la distribución de Poisson se utiliza para contar eventos en un intervalo fijo de tiempo o espacio.\nVolviendo a los ejemplos presentados:\n\nEl número de pichirilos que podemos encontrar cada 20 carros que veamos con una tasa de exito aproximada del 1%.\n\n\nSigue una distribución binomial con 200 intentos (n) y probaibilidad de éxito del 1% (p) lo que significa que:\na. Los resultados del fenómeno son enteros, es decir que nuestra variable aleatoria es discreta\nb. El valor esperado de esta función es:\n\\[\nE(X) = n \\times p = 200 \\times 0.01 =2\n\\]\nc. Y su varianza es:\n\\[\nV(X) = n \\times p \\times q = 200 \\times 0.01 \\times(1-0.01)=1.98 ...\n\\]\nEn nuestro caso esperamos encontrar 2 pichirilos cada 200 carros.\n\n\n\nFijate que aquí en el caso de los pichirilos es importante señalar algunas condiciones como por ejemplo el lugar, la hora etc, de la misma manera en el caso de la secretaría es necesario hacernos la pregunta ¿A qué hora del día? ya que en ambos casos hay horas del día en que por hora pico cambian las condiciones lo cual hara difícil que realicemos replicas los resultados y por ende nuestras conclusiones de nuestros experimentos sean inconclusas.\n\n\nEl número de estudiantes atendidos por la secretaría del departamento de ciencias cada hora con una tasa promedio de 2 estudiantes.\n\n\nSigue una distribución de Poisson con el parametro \\(\\lambda = 2\\)\na. Los resultados del fenómeno son enteros, es decir que nuestra variable aleatoria es discreta\nb. El valor esperado de esta función es:\n\\[\nE(X) = \\lambda\n\\]\nc. Y su varianza es:\n\\[\nV(X) = \\sqrt{\\lambda}\n\\]\nEn nuestro caso esperamos encontrar 2 estudiantes por hora. (Valga la redundancia)\n\n\n\nDistribución de Probabilidad\nLa distribución de probabilidad es una descripción matemática de cómo se distribuyen los resultados en un experimento aleatorio. En el caso de nuestra actividad con los dados, cada lanzamiento es un experimento, y la suma de los dados es el resultado cuya distribución estamos interesados en explorar.\nMediante la simulación de 10,000 lanzamientos, hemos creado una muestra considerable que refleja cómo se distribuyen los resultados de la suma de dos dados. Esta distribución nos permite ver cuán frecuente es cada resultado posible (del 2 al 12) en nuestros experimentos.\nPara visualizar esta distribución, podemos emplear gráficos como histogramas usando bibliotecas de Python como matplotlib o seaborn. Un histograma mostraría claramente cuáles sumas son más probables (por ejemplo, 7 es el resultado más común en la suma de dos dados) y cuáles son menos comunes (como 2 o 12).\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n# Establecer la seed para reproducibilidad\nnp.random.seed(0)\n\n# Generar 10,000 lanzamientos para cada dado\ndado1 = np.random.randint(1, 7, 10000)\ndado2 = np.random.randint(1, 7, 10000)\n\n# Crear un DataFrame con los resultados\ndf_dados = pd.DataFrame({'Dado1': dado1, 'Dado2': dado2})\ndf_dados['Suma'] = df_dados['Dado1'] + df_dados['Dado2']\n\n# Análisis estadístico básico\nmedia = df_dados['Suma'].mean()\nmediana = df_dados['Suma'].median()\nmoda = df_dados['Suma'].mode()[0]\n\nprint(f\"Media: {media}, Mediana: {mediana}, Moda: {moda}\")\n# Crear un histograma de la suma de los dados\nplt.hist(df_dados['Suma'], bins=range(2, 13), edgecolor='black', align='left')\nplt.xlabel('Suma de los Dados')\nplt.ylabel('Frecuencia')\nplt.title('Distribución de la Suma de Dos Dados')\nplt.show()\n\nMedia: 7.0048, Mediana: 7.0, Moda: 7\n\n\n\n\n\n\n\nEstimaciones de los Momentos a Partir de Réplicas\n\nUso de Proporciones para Estimar Probabilidades\nUna forma poderosa de estimar probabilidades es a través de la proporción de ocurrencias de un evento en nuestras réplicas. En nuestro experimento de 10,000 lanzamientos de dados, cada resultado de la suma (del 2 al 12) tiene una cierta cantidad de ocurrencias. La proporción de cualquier suma específica (por ejemplo, la suma es igual a 7) sobre el total de lanzamientos nos da una estimación de la probabilidad de ese resultado.\n\n\nEjemplo con la Simulación de Dados\nSi queremos calcular la probabilidad de obtener una suma de 7, contaríamos cuántas veces ocurre esto en nuestros 10,000 lanzamientos y lo dividiríamos por 10,000. Este método nos proporciona una estimación empírica de la probabilidad basada en los datos simulados, la cual puede ser comparada con la probabilidad teórica calculada matemáticamente.\n\n# Podemos hacer en dos pasos:\n\ndf_grouped = df_dados.groupby(\"Suma\").size().reset_index(name = \"Frecuencia\")\n\ntotal = df_grouped['Frecuencia'].sum()\n\ndf_grouped['Porcentaje'] = df_grouped['Frecuencia'] / total * 100\n\nprint(df_grouped)\n\n    Suma  Frecuencia  Porcentaje\n0      2         261        2.61\n1      3         548        5.48\n2      4         859        8.59\n3      5        1122       11.22\n4      6        1367       13.67\n5      7        1675       16.75\n6      8        1401       14.01\n7      9        1106       11.06\n8     10         805        8.05\n9     11         575        5.75\n10    12         281        2.81\n\n\nUna alternativa puede ser la siguiente:\n\n# O con un condicional:\n\ncondicion = abs(df_dados['Suma']) == 7\n\nprob_dif_2 = np.mean(condicion)\n\nprob_dif_2\n\n0.1675\n\n\n\n\n\nEstimadores y la Ley de los Grandes Números\n\nQué es un Estimador\nUn estimador en estadísticas es una regla o un método para calcular una estimación de una cantidad desconocida basándose en observaciones. En el contexto de nuestra simulación de dados, estamos utilizando un estimador para calcular la probabilidad de un resultado específico de la suma de los dados.\n\n\nFórmula de un Estimador de Proporción\nEl estimador de proporción que estamos utilizando se puede expresar como:\n\\[ \\hat{p} = \\frac{\\text{número de veces que ocurre el evento}}{\\text{total de réplicas}} \\]\nEn esta fórmula, () es nuestra estimación de la probabilidad, y se calcula dividiendo el número de veces que un evento específico ocurre (por ejemplo, obtener una suma de 7) por el número total de lanzamientos (o réplicas) que hemos simulado.\nEsta forma de estimar probabilidades está intrínsecamente relacionada con la Ley de los Grandes Números. Esta ley afirma que, a medida que aumenta el número de ensayos en un experimento aleatorio, la media de los resultados observados tiende a acercarse a la media esperada teóricamente. Por lo tanto, con suficientes réplicas, nuestra estimación () se acercará a la probabilidad real del evento.\n\n\n\nIntroducción a la Distribución Normal\nLa distribución normal, también conocida como la distribución gaussiana, es una de las distribuciones más importantes en estadística. Se caracteriza por su forma de campana simétrica y es fundamental en muchas áreas de la estadística y la probabilidad. La distribución normal se utiliza para modelar una variedad de fenómenos naturales y sociales, y es esencial en la teoría del muestreo y la inferencia estadística.\n\n\nFórmulas y Conceptos Clave\n\nFunción de Distribución y Función de Densidad\nVamos a ir explicando los conceptos descritos en conjunto con el código de Python. Primero describamos la función de densidad de la distribución normal la cual está dada por la formula: \\[\nf(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{1}{2} \\left(\\frac{x - \\mu}{\\sigma}\\right)^2}\n\\]\ndonde \\(\\mu\\) es la media y \\(\\sigma\\) es la desviación estándar. Está función de probabilidad entra dentro de la familia de funciones paramétricas siendo estos dos momentos de la distribución los parametros que dan forma a la función de distribución normal.\n\n\nMedia y Varianza\nLa media \\(\\mu\\) es uno de los momentos de la distribución normal el cual por lo general está ubicado en la mitad de la distribución de valores. Si una variable aleatoria sigue una distribución normal, los valores que “más probables” son aquellos que estén cerca de la media \\(\\mu\\) cuando la varianza \\(\\sigma^2\\) o medida de distribución tiene valores pequeños.\nPara estimar la media \\(\\mu\\) empleamos el promedio aritmético:\n\\[\n\\hat{E(X)} = \\bar{X} = \\sum_{i = 1}^{N}\\frac{x_i}{N}\n\\]\nMientras que un estimador para la varianza \\(\\sigma^2\\) es la suma al cuadrado de la desviación de los valores con respecto a la media:\n\\[\n\\hat{V(X)} = s^2 = \\sum_{i = 1}^{N}\\frac{(x_i - \\bar{X})^2}{N-1}\n\\]"
  },
  {
    "objectID": "clase_4.html#propiedades-de-un-estimador",
    "href": "clase_4.html#propiedades-de-un-estimador",
    "title": "Variables aleatorias, funciones de distribución y estimación de parametros",
    "section": "Propiedades de un estimador",
    "text": "Propiedades de un estimador\nEn econometría, un estimador es una regla o método para calcular una estimación de un parámetro desconocido basado en datos observados. Hay varias propiedades clave que un buen estimador debe tener para ser considerado fiable y útil:\n\nInsesgado o No Sesgado (Unbiasedness): Un estimador se considera insesgado si su valor esperado es igual al parámetro real que se intenta estimar. Esto significa que en promedio, a lo largo de muchas muestras, el estimador acierta el valor verdadero del parámetro.\nEficiencia (Efficiency): Entre varios estimadores insesgados, el más eficiente es aquel con la menor varianza. Esto significa que proporciona la estimación más precisa posible (menos dispersa alrededor del valor verdadero) dada una cierta cantidad de datos.\nConsistencia (Consistency): Un estimador es consistente si, a medida que el tamaño de la muestra aumenta, converge en probabilidad hacia el verdadero valor del parámetro que se está estimando. Esto implica que el estimador se vuelve cada vez más preciso a medida que se dispone de más datos.\nSuficiencia (Sufficiency): Un estimador es suficiente si utiliza toda la información relevante que los datos aportan sobre el parámetro. Un estimador suficiente resume los datos de tal manera que toda la información sobre el parámetro que se desea estimar está contenida en el estimador.\nRobustez (Robustness): Un estimador robusto mantiene sus propiedades (como ser insesgado y consistente) incluso cuando se violan algunas de las suposiciones en las que se basa el modelo. Esto es importante en la práctica, ya que los datos reales a menudo no cumplen con todas las suposiciones teóricas del modelo.\nNormalidad Asintótica (Asymptotic Normality): En el caso de estimadores consistentes, se desea que, a medida que el tamaño de la muestra aumenta, la distribución del estimador se aproxime a una distribución normal. Esto facilita la realización de inferencias estadísticas, como pruebas de hipótesis y la construcción de intervalos de confianza.\n\nEstas propiedades son ideales y, en la práctica, puede ser difícil encontrar un estimador que cumpla con todas ellas en una situación dada. Por lo tanto, los econometristas a menudo tienen que hacer compromisos basados en el contexto específico y las limitaciones de los datos disponibles.\n\nMediana y Rango Intercuartílico\nUn inconceniente del trabajo con variables aleatorias es que no siempre el mundo es color de rosa como en la distribución normal. Usualmente nos encontramos con presencia de valores atípicos los cuales tienen un impacto significativo en las estimaciones de \\(\\mu\\) y \\(\\sigma^2\\).\nEn ese sentido existen métricas que nos dan mejor idea de lo que sucede dentro de nuestra variable estos son:\n\nLa mediana la cual es el valor bajo el cual se encuentra el 50% de la población. Por ejemplo si analizamos el salario básico unificado, más de la mitad de la población del Ecuador tiene un salario menor a 600 dólares lo cual nos indicaría que la mediana está cerca de ese valor. Otro nombre para la mediana es percentil 50.\nContinuando con los percentiles, como se mencionó la varianza es suceptible a valores que están muy lejos de la media. Por ende un mejor indicador de la dispersión de los datos es la diferencia entre el percentil 75 y el percentil 25 el cual se conoce como rango intercuartilico. Este valor indica entre que valores se encuenta el 50% de la población con valores más frecuentes.\n\n\n\nConstrucción de un gráfico con los momentos de una distribución\n\nHistograma de 10,000 Números Aleatorios\nPrimero, generamos una muestra de 10,000 números con una distribución normal usando numpy y la visualizamos en un histograma.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import norm\n\ndata = np.random.normal(400, 200, 10000)\nplt.hist(data, bins=30, edgecolor='black', density=True)\nplt.title('Histograma de Distribución Normal')\n\nText(0.5, 1.0, 'Histograma de Distribución Normal')\n\n\n\n\n\nAñadir la Línea de la Función de Densidad Sobreponemos la curva de la función de densidad normal utilizando los valores de media y desviación estándar.\n\n\nplt.hist(data, bins=30, edgecolor='black', density=True)\n\nplt.title('Función de Densidad Distribución Normal')\n\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = norm.pdf(x, 400, 200)\nplt.plot(x, p, 'k', linewidth=2)\n\n\n\n\n\nIncorporar la Media Añadimos una línea vertical para indicar la media de la distribución.\n\n\nplt.hist(data, bins=30, edgecolor='black', density=True)\n\nplt.title('Función de Densidad Distribución Normal')\n\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = norm.pdf(x, 400, 200)\nplt.plot(x, p, 'k', linewidth=2)\n\nplt.axvline(data.mean(), color='red', linestyle='dashed', linewidth=1)\nplt.legend(['Función de Densidad', 'Media'])\n\n&lt;matplotlib.legend.Legend at 0x1e0275ef9d0&gt;\n\n\n\n\n\n\nAñadir la Mediana Mostramos la mediana en el gráfico con otra línea vertical.\n\n\nplt.hist(data, bins=30, edgecolor='black', density=True)\n\nplt.title('Función de Densidad Distribución Normal')\n\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = norm.pdf(x, 400, 200)\nplt.plot(x, p, 'k', linewidth=2)\n\nplt.axvline(data.mean(), color='red', linestyle='dashed', linewidth=1)\n\nplt.axvline(np.median(data), color='green', linestyle='dashed', linewidth=1)\n\nplt.legend(['Función de Densidad', 'Media', 'Mediana'])\n\n&lt;matplotlib.legend.Legend at 0x1e0276a5690&gt;\n\n\n\n\n\n\nDesviación Estándar y Rango Intercuartílico Finalmente, representamos la desviación estándar con lineas purpura y el rango intercuartílico como una sombra gris en el gráfico.\n\n\nplt.hist(data, bins=30, edgecolor='black', density=True)\n\nplt.title('Función de Densidad Distribución Normal')\n\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = norm.pdf(x, 400, 200)\nplt.plot(x, p, 'k', linewidth=2)\n\nplt.axvline(data.mean(), color='red', linestyle='dashed', linewidth=1)\n\nplt.axvline(np.median(data), color='green', linestyle='dashed', linewidth=1)\nplt.axvline(data.mean() + data.std(), color='blue', linestyle='dashed', linewidth=1)\nplt.axvline(data.mean() - data.std(), color='purple', linestyle='dashed', linewidth=1)\nplt.axvspan(np.percentile(data, 25), np.percentile(data, 75), alpha=0.3, color='gray')\nplt.legend(['Función de Densidad', 'Media', 'Mediana', 'Desviación Estándar', 'IQR'])\n\n&lt;matplotlib.legend.Legend at 0x1e027525690&gt;\n\n\n\n\n\nCon estos pasos, hemos construido un gráfico que no solo muestra la distribución de los datos, sino también sus características estadísticas clave."
  },
  {
    "objectID": "clase_6.html#resumen",
    "href": "clase_6.html#resumen",
    "title": "Exploración y visualización de datos con Matplotlib y Seaborn en Python",
    "section": "Resumen",
    "text": "Resumen\nEste capítulo aborda dos importantes bibliotecas de visualización de datos en Python: Matplotlib y Seaborn.\nMatplotlib, una librería open source creada en 2002 por John Hunter, es fundamental para la visualización de datos en Python, ofreciendo gráficos en dos dimensiones y visualizaciones dinámicas e interactivas. PyPlot, un módulo de Matplotlib, facilita la adición de elementos como líneas, imágenes o texto en los gráficos.\nSeaborn, construido sobre Matplotlib y creado por Michael Waskom, busca simplificar la creación de gráficos estadísticos. Sus características clave incluyen:\nEstética agradable con opciones de color y temas predefinidos. Integración con pandas, permitiendo el uso eficiente de DataFrames. Enfoque en visualizaciones estadísticas para análisis de datos. Facilidad de uso con funciones especializadas. Alta capacidad de personalización. Ambas bibliotecas tienen documentación extensa y valiosa para su uso óptimo. La integración con pandas es especialmente útil, como se muestra en los ejemplos de código.\nEl capítulo continúa con una discusión sobre variables numéricas y categóricas, seguido de ejemplos prácticos para crear diversos tipos de gráficos utilizando Matplotlib y Seaborn, como gráficos de barras, de burbuja, radar, de pastel, de matriz y de líneas. Estos ejemplos muestran la versatilidad y eficiencia de ambas bibliotecas en la representación gráfica de datos.\nFinalmente, se destaca la importancia de la selección de colores y etiquetas en la presentación de gráficos. Se aborda cómo el color puede influir en la interpretación y percepción de los datos. Seaborn ofrece diversas paletas de colores (cualitativas, secuenciales y divergentes) para adaptarse a diferentes tipos de datos y visualizaciones. La librería Palettable también se menciona como una alternativa para la selección de paletas de colores. La elección correcta de etiquetas es crucial para evitar ambigüedades y facilitar la comprensión de los gráficos.\nEn resumen, este capítulo proporciona una comprensión integral de Matplotlib y Seaborn, destacando sus capacidades, integración con otras bibliotecas y su rol esencial en la visualización de datos en Python."
  },
  {
    "objectID": "clase_6.html#matplotlib-y-seaborn",
    "href": "clase_6.html#matplotlib-y-seaborn",
    "title": "Exploración y visualización de datos con Matplotlib y Seaborn en Python",
    "section": "Matplotlib y Seaborn",
    "text": "Matplotlib y Seaborn\nMatplotlib es una librería open source de Python con la que se pueden crear gráficos en dos dimensiones, visualizaciones dinámicas, interactivas, muy importante para la visualización de datos al momento de realizar un análisis. Fue creada en 2002 por John Hunter, neurobiólogo, quien buscaba una forma de replicar la creación gráfica de Matlab en Python. A pesar del fallecimiento de su creador en el año 2012, esta al ser open source, ha ido mejorando gracias al aporte de la comunidad hasta convertirse en una gran alternativa a Matlab.\nCon Matplotlib también se habla de PyPlot, el cual es un módulo que nos presenta funciones para añadir elementos a nuestros gráficos como líneas, imágenes o texto, posee una interfaz sencilla y es muy usado.\nSeaborn es una biblioteca de visualización de datos en Python que está construida sobre Matplotlib. Fue creado por Michael Waskom y tiene como objetivo hacer que la creación de gráficos estadísticos atractivos y informativos sea más fácil. Algunos puntos a resaltar serían:\n\nEstética agradable: Seaborn proporciona una interfaz de alto nivel para crear gráficos estadísticos atractivos y con estilo. Ofrece opciones de color predefinidas y temas, lo que facilita la creación de visualizaciones agradables sin tener que personalizar todos los detalles manualmente.\nIntegración con pandas: Seaborn se integra bien con pandas, otra biblioteca popular de Python para manipulación y análisis de datos. Puedes pasar fácilmente DataFrames de pandas a funciones de Seaborn para crear visualizaciones rápidas.\nVisualizaciones estadísticas: Seaborn está diseñado específicamente para la visualización de datos estadísticos. Ofrece funciones para visualizar distribuciones univariadas y bivariadas, matrices de correlación, y más. Esto lo hace especialmente útil en entornos de análisis de datos y estadísticas.\nFacilidad de uso: Seaborn simplifica la creación de gráficos complejos mediante funciones especializadas que facilitan la generación de diagramas de dispersión, diagramas de caja, gráficos de barras, entre otros.\nPersonalización: Aunque Seaborn proporciona una estética agradable de forma predeterminada, también permite una gran personalización. Puedes ajustar colores, estilos y otros aspectos visuales según tus preferencias.\n\nCabe recordar que tanto Matplotlib como Seaborn tienen su respectiva documentación que vale la pena revisar, la misma ayudará a entender de mejor manera cómo usar las cientos de funciones que poseen cada una.\nAhora veamos como llamar a estas librerias\n\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn import datasets\n\n\nNota: Pandas es una librería que se integra de muy buena manera con estas dos, puesto que el uso de DataFrames es muy versátil con esta librería."
  },
  {
    "objectID": "clase_6.html#variables-numéricas-y-categóricas",
    "href": "clase_6.html#variables-numéricas-y-categóricas",
    "title": "Exploración y visualización de datos con Matplotlib y Seaborn en Python",
    "section": "Variables numéricas y categóricas",
    "text": "Variables numéricas y categóricas\nEn esta parte vamos a dar un pequeño recordatorio de qué son cada una de las variables\n\nNuméricas: Son aquellas que pueden ser representadas por cantidades numéricas, son discretas o continuas. Algunos ejemplos son la edad, el peso o la altura.\nCategóricas: Son aquellas que representan diferentes grupos y no tienen un orden entre ellas. Pueden ser el género, etnia, etc\n\nAhora veamos cómo crear estos gráficos usando ambas librerias.\n\nGráfico de barras\nEste gráfico es recomendable usarlo cuando comparas variables numéricas con una categórica. Es muy bueno para mostrar diferencias entre los datos de la variable categórica.\n\n# Creamos 3 variables \nnombres = [\"Edgar\", \"María\", \"Pablo\",\"Alex\"]\nedad = [26,25,24,28]\nestatura = [171,170,178,185]\n\n#Vamos a crear un gráfico simple de barras, que muestre el nombre de cada uno y sus distintas edades\n\nplt.bar(nombres, edad, color='red')\n\n# Agregamos etiquetas y el título\n\nplt.xlabel('Nombre') #Eje x\nplt.ylabel('Edad') #Eje y\nplt.title('Diagrama de Barras') #Titulo\n\n# Mostrar el diagrama de barras\nplt.show()\n\n\n\n\nListo, hemos creado nuestro primer gráfico. Ahora, usaremos otra librearia para crear lo mismo\n\n#Creamos un datafrme con los datos anteriores\ndatos = {\n  'nombres': [\"Edgar\", \"María\", \"Pablo\",\"Alex\"],\n  'edad' : [26,25,24,28],\n  'estatura': [171,180,189,165]\n}\n\ndf= pd.DataFrame(datos)\n\n# Se crea el gráfico\nsns.barplot(x='nombres', y='edad', data= df)\n\n#Titulos y más\nplt.xlabel('Nombre')\nplt.ylabel('Edad')\nplt.title('Diagrama de Barras')\nplt.show()\n\n\n\n\nComo podemos observar, usando la primera opción no fue necesario crear un dataframe, sin embargo en la segunda tuvimos que hacer uso del mismo y además saber los nombres de las variables para saber qué vamos a graficar, por tanto el uso de una u otra dependerá de cada uno. Veamos más ejemplos de otros gráficos muy usados en el data storytelling.\n\n\nGráfico de barras agrupado\nEs similar a un gráfico de barras solo que las categorías pueden mostrar más de una variable. Es recomendable usarlo cuando quieres contrastar dos variables relacionadas.\n\n# Usando matplotlib el código es el siguiente\nx= np.arange(4) #creamos un arreglo con numpy con el numero de variables, 4 nombres\nancho = 0.40\n  \n# Se crean ambos gráficos \nplt.bar(x-0.2, edad, ancho)\nplt.bar(x+0.2, estatura, ancho)\nplt.show()\n\n\n\n\n\n#Usando Seaborn tenemos\n\nsns.barplot(data=df, x='nombres', y='edad', label='edad', color='blue')\nsns.barplot(data=df, x='nombres', y='estatura', label='estatura', color='orange', bottom=df['edad'])\n\n# Configurar etiquetas y leyenda\nplt.xlabel('Nombres')\nplt.ylabel('Valores')\nplt.title('Gráfico de Barras Agrupadas usando Seaborn')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x22435b01e10&gt;\n\n\n\n\n\n\n\nGráfico de burbuja\nEs un gráfico que muestra datos en tres dimensiones en un plano de dos, la tercera sería el tamaño de la burbuja, sirve para mostrar la difencia entre conjuntos de la misma variable u otras.\n\n#Leemos el archivo de datos\n\narchivo_excel = 'co2.xlsx'\n\ndatos_excel = pd.read_excel(archivo_excel)\n\ncol1=datos_excel['Country Name'].head(4)\ncol2= np.arange(1,5)\ntamanio= datos_excel['2020'].head(4)\n\nplt.scatter(col1, col2, s=tamanio, alpha=0.5)\n\n# Personalizar el gráfico\nplt.title('Emiciones de CO2 en 2020')\nplt.xlabel('Países')\n\n# Mostrar el gráfico\nplt.show()\n\n\n\n\n\n\nGráfico de araña (radar chart)\nEste gráfico es útil para comparar varias categorías o variables en relación a un conjunto de ejes en común.\n\n# usaremos nuestro archivo de emisiones de Co2 \ndatos1 = datos_excel.loc[8,['1990','2000','2010','2020']] \ndatos2 = datos_excel.loc[12,['1990','2000','2010','2020']] \ndatos11 = datos1.tolist() \ndatos22 = datos2.tolist() \n# Primer gráfico \ncategorias1 = ['1990', '2000', '2010', '2020'] \nvalores1 = datos11 \n# Segundo gráfico \ncategorias2 = ['1990', '2000', '2010', '2020'] \nvalores2 = datos22 \n# ángulo de cada eje\nangulos = np.linspace(0, 2 * np.pi, len(categorias1), endpoint=False) \n# Asegurar que el gráfico es circular\nvalores1 += valores1[:1]\nvalores2 += valores2[:1]\nangulos = np.concatenate((angulos, [angulos[0]]))\n\n# Crear subgráficos\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\n\n# Creamos el primer gráfico de telaraña\nax.plot(angulos, valores1, label='Antigua y Barbuda', marker='o')\nax.fill(angulos, valores1, alpha=0.25)\n\n# Creamos el segundo gráfico de telaraña\nax.plot(angulos, valores2, label='Burundi', marker='o')\nax.fill(angulos, valores2, alpha=0.25)\n\n# Personalizar el gráfico\nax.set_xticks(angulos[:-1])\nax.set_xticklabels(categorias1)\n\n# Mostrar leyenda\nax.legend()\n\nplt.show()\n\n\n\n\n\n\nDiagrama de pastel\nSirven para comparar porcentajes de cierta variable y es recomendable usarlo cuando se quiera resaltar el mayor porcentaje de datos en nuestras variables.\n\n#creamos nuevas variables \nobjeto = ['Detergente A', 'Detergente B', 'Detergente C', 'Detergente D']\ncantidad = [34,26,11,30]\n\nplt.pie(cantidad, labels=objeto, autopct='%1.1f%%', startangle=90, colors=['lightblue', 'lightgreen', 'lightcoral','red'])\n\n# Configuración adicional\nplt.axis('equal')  \nplt.title('Diagrama de Pastel')\n\n# Mostrar el gráfico\nplt.show()\n\n\n\n\n\n#Usando Seaborn, junto con matplotlib\ncolores = sns.color_palette(\"bright\")\nplt.pie(cantidad, labels=objeto, colors=colores, autopct='%1.1f%%')\nplt.title('Diagrama de Pastel')\nplt.show()\n\n#Aqui hemos usado la paleta de colores de seaborn \n\n\n\n\n\n\nGráfico de lineas\nEs muy útil para mostrar como evolucionan los datos a través del tiempo.\n\n#Crearemos variables con el PIB del país a traves de los años\nanio = [2014,2015,2016,2017,2018,2019,2020]\npib = [101.7,99.3,99.9,104.3,107.6,108.1,98.8]\n\nplt.plot(anio, pib, marker='o', linestyle='-', color='b', label='Datos de ejemplo')\n\n# Titulos y más\nplt.title('Gráfico de Líneas')\nplt.xlabel('Año')\nplt.ylabel('PIB')\nplt.legend()  # Muestra la leyenda si se proporcionan etiquetas\n\n# Mostrar el gráfico\nplt.show()\n\n\n\n\n\n# Usando Seaborn\nsns.set(style=\"whitegrid\")\nsns.lineplot(x=anio, y=pib, marker='*', color='red', label='Datos de ejemplo')\n\n# Títulos y más\nplt.title('Gráfico de Líneas con Seaborn')\nplt.xlabel('Eje X')\nplt.ylabel('Eje Y')\nplt.legend()\n\n# Mostrar el gráfico\nplt.show()"
  },
  {
    "objectID": "clase_6.html#presentación-de-gráficos-paletas-de-color-y-etiquetas",
    "href": "clase_6.html#presentación-de-gráficos-paletas-de-color-y-etiquetas",
    "title": "Exploración y visualización de datos con Matplotlib y Seaborn en Python",
    "section": "Presentación de gráficos: paletas de color y etiquetas",
    "text": "Presentación de gráficos: paletas de color y etiquetas\nEn el data storytelling el color juega un papel muy importante ya que estos deben ser relevantes. La percepción del color se estudia en varias disciplinas, desde psicología hasta optometría. La interpretación del significado del color esta fuertemente influenciada por factores ajenos a uno, como pueden ser la religión o las costumbres.\nSi lo que deseamos es comunicar de manera efectiva alguna información es escencial comprender las necesidades, costumbres y actitudes del receptor de la misma. Es así que para seleccionar el color podemos guiarnos por:\n\nContraste en los colores:\nAl momento de escoger el color debemos darnos cuenta en como estos ayudan a resaltar una idea o mostrar una diferencia.\nMetáfora en los colores y degradados:\nLos colores como una metáfora se refiere a cuando los usamos con un significado en la sociedad, por ejemplo el rojo para decir parar o el verde para seguir. El degradado se refiere a escalas que crecen o decrecen según algunos valores.\n\nPara ahondar más en el tema hablemos de las paletas de colores en seaborn, esta librería facilita el uso de los mismos y nos da pistas de cómo encontrar la mejor solución para nuestro trabajo. Podemos clasificar las paletas en tres categorías:\n\nPaletas cualitativas\nPaletas secuenciales\nPaletas divergentes\n\n\nPaletas cualitativas\nEs recomendable usar las mismas con datos categóricos, ya que su variación se enceuntra en el componente del tono. Uno puede ver los diez tonos usando el siguiente código\n\nsns.color_palette()\n\n\n\n\nUno puede comparar esta paleta de colores con la derterminada de matplotlib la cual es más intensa, además seaborn cuenta con 6 variaciones de la misma.\n\n\nPaletas secuenciales\nEsta paleta es apropiada cuando los datos varían desde valores bajos hasta valores altos. En donde varía esta paleta es en la luminancia. Existen varias paletas con esta característica en Seaborn por lo que tenemos varias opciones para elegir, a continuación mostramos algunas que se pueden visualizar\n\nsns.color_palette(\"rocket\", as_cmap=True)\nsns.color_palette(\"mako\", as_cmap=True)\n#Paletas de una solo color\nsns.light_palette(\"seagreen\", as_cmap=True)\nsns.dark_palette(\"#69d\", reverse=True, as_cmap=True)\n\nblend  underbad over \n\n\n\n\nPaletas divergentes\nEstas se usan para datos en donde los valores altos como los bajos son de interés, a menudo con punto medio sin tanta importancia. Aquí debemos tener en cuenta que debe haber dos tonos dominantes, uno en cada polo.\nPara ver lo que nos ofrece Seaborn podemos usar el siguiente código\n\nsns.color_palette(\"vlag\", as_cmap=True)\nsns.color_palette(\"icefire\", as_cmap=True)\n\nicefire  underbad over \n\n\nCabe destacar que toda esta información se encuentra más detallada en la página oficial del paquete seaborn\n\nhttps://seaborn.pydata.org/tutorial/color_palettes.html\n\nExiste otra librería que nos deja escocger nuestras paletas de colores, esta es `Palettable`. Aquí esta un link directo a su página en github\n\nhttps://jiffyclub.github.io/palettable/\n\nHablemos ahora de las etiquetas de los gráficos, las cuales facilitan la compresión del mismo y sirven de guía para quien va dirigido. Por la misma razón debemos escoger etiquetas que no se presten para ambigüedades. Al momento de poner nuestras etiquetas debemos pensar que otras interpetaciones puede tener nuestro gráfico y luego de ello escoger la mejor que resalte lo que deseamos expresar."
  },
  {
    "objectID": "clase_7.html#de-cervezas-y-datos",
    "href": "clase_7.html#de-cervezas-y-datos",
    "title": "Pruebas de hipótesis para la media entre grupos en Python",
    "section": "De cervezas y Datos",
    "text": "De cervezas y Datos\nLa prueba T de Student es una herramienta estadística fundamental, y su historia es tan interesante como su aplicación. Fue desarrollada por William Sealy Gosset, un químico y matemático que trabajaba en la cervecería Guinness en Dublín, Irlanda, a principios del siglo XX.\nGosset se enfrentaba a un problema común en el control de calidad de la cerveza: tenía que trabajar con muestras pequeñas debido a limitaciones prácticas, pero necesitaba métodos eficaces para analizar la calidad y la consistencia del producto. Los métodos estadísticos existentes en esa época eran adecuados para muestras grandes, pero no para las pequeñas muestras con las que trabajaba Gosset.\nPara abordar este problema, Gosset desarrolló la prueba T, que era especialmente adecuada para estimar la media de una población y comparar medias entre dos grupos pequeños. Publicó su trabajo en 1908 bajo el seudónimo “Student” para proteger su identidad y la de su empleador, Guinness, que prefería mantener en secreto sus métodos de control de calidad. El artículo se tituló “The Probable Error of a Mean” y apareció en la revista “Biometrika”.\nLa prueba T de Student fue revolucionaria porque permitió a los investigadores hacer inferencias estadísticamente válidas a partir de muestras pequeñas. Esto tuvo un impacto significativo en campos como la agricultura, la ingeniería y, por supuesto, la calidad de los productos como la cerveza.\nCon el tiempo, la prueba T se expandió en varias variantes, como la prueba T de una muestra, la prueba T de dos muestras independientes, y la prueba T de muestras pareadas. Cada una de estas variantes se utiliza en circunstancias específicas, pero todas se basan en el principio fundamental de comparar medias y evaluar diferencias en términos de la variabilidad observada en los datos.\nLa prueba T de Student sigue siendo una herramienta esencial en la estadística y es ampliamente utilizada en la investigación científica, la ingeniería, la economía, la psicología y muchos otros campos para la toma de decisiones basada en datos."
  },
  {
    "objectID": "clase_7.html#pruebas-de-hipótesis",
    "href": "clase_7.html#pruebas-de-hipótesis",
    "title": "Pruebas de hipótesis para la media entre grupos en Python",
    "section": "Pruebas de hipótesis",
    "text": "Pruebas de hipótesis\nLas pruebas de hipótesis son una herramienta muy útil en la estadística inferencial, la cual usamos para tomar una decisión sobre una población basandonos en una muestra de la misma. Son usadas para evaluar afirmaciones o suposiciones sobre una característica desconocida de una población.\nAquí introducimos el concepto de hipótesis nula e hipótesis alternativa , nivel de significación y errores Tipo I y Tipo II:\n\nHipótesis nula ( \\(H_0\\) ): Es lo que queremos poner a prueba, lo que se cree en un inicio es verdad.\nHipótesis alternativa ( \\(H_1\\) ): Es lo que se acepta si los datos nos dan evidencia para rechazar la hipótesis nula, la negación de \\(H_0\\).\nNivel de significación ( \\(\\alpha\\) ): Representa la probabilidad de cometer un error de tipo I al rechazar incorrectamente la hipótesis nula.\nError Tipo I: Rechazar incorrectamente la hipóstesis nula cuando es verdadera.\nError Tipo II: No rechazar la hipótesis nula cuando es falsa.\n\nDefinidas las hipótesis a contrastar se realiza una prueba donde se obtendrá un valor llamado valor p. Veremos cuando este es significativo (por lo general mayor que 0.05) y cómo se le usa para rechazar \\(H_0\\). Otro punto a tomar en cuenta es nuestro tamaño de muestra, por lo general un número mayor a 30 es considerado grande y es posible usar algunos resultados estadísticos para aproximar nuestros resultados, si esto no ocurre tendremos que usar otras técnicas, como el uso de la prueba T de student, antes mencionada."
  },
  {
    "objectID": "clase_7.html#casos-cuando-existe-varianza-en-los-grupos",
    "href": "clase_7.html#casos-cuando-existe-varianza-en-los-grupos",
    "title": "Pruebas de hipótesis para la media entre grupos en Python",
    "section": "Casos cuando existe varianza en los grupos",
    "text": "Casos cuando existe varianza en los grupos\nAunque los paquetes hagan la mayoría de trabajo no esta de más saber los distintos casos que existen en estas pruebas de hipótesis\n\nCaso cuando la varianza poblacional es desconocida\nEs es el caso más común, en este desconocemos la varianza poblacional y por lo general se usa una distribucion \\(t\\) de Student en lugar de una normal.\nEn este caso usamos un estadístico de prueba \\(t=\\frac{\\bar{x}-u_0}{s/\\sqrt{n}}\\) donde \\(s\\) es la desviación estándar y n el tamaño de la muestra y el cual sigue una distribución de Student con \\(n-1\\) grados de libertad.\nCabe recalcar que lo antes mencionado es solamente cuando tenemos una muestra, veamos ahora casos cuando queremos comparar dos muestras.\n\n\nCaso cuando las varianzas son desconocidas pero iguales\nEste caso es de los casos más comunes, aquí tenemos dos medias cuyas varianzas son desconocidas. Primero debemos hacer una prueba F \\((F=\\frac{s_1^2}{s_2^2})\\) para probar si las varianzas son iguales y luego podemos usar el estadístico siguiente \\[\nt=\\frac{(\\bar{x}_1-\\bar{x}_2)-(u_1-u_2)}{s_p\\sqrt{1/n_1+1/n_2}}\n\\] donde \\[\ns^2_p=\\frac{s_1^2(n_1-1)+s_2^2(n_2-1)}{n_1+n_2-2}\n\\] con grados de libertad dados por \\(v=n_1+n_2-2\\)\n\n\nCaso cuando las varianzas son desconocidas pero distintas\nSi luego de realizar la prueba F sobre las varianzas tenemos que no se pueden considerar iguales tenemos que uasr el siguiente estadístico \\[\nt=\\frac{(\\bar{x}_1-\\bar{x}_2)-(u_1-u_2)}{\\sqrt{s_1^2/n_1+s_2^2/n_2}}\n\\] donde los grados de libertad vienen dados por \\[\nv=\\frac{(s_1^2/n_1+s_2^2/n_2)^2}{\\frac{(s_1^2)^2}{(n_1-1)n_1^2}+\\frac{(s_2^2)^2}{(n_2-1)n_2^2}}\n\\] y este valor rara vez será un número entero, por lo que será redondeado al menor entero más cercano."
  },
  {
    "objectID": "clase_7.html#biblioteca-scipy",
    "href": "clase_7.html#biblioteca-scipy",
    "title": "Pruebas de hipótesis para la media entre grupos en Python",
    "section": "Biblioteca Scipy",
    "text": "Biblioteca Scipy\nAhora que conocemos algunos casos que se nos podrían presentar vamos a ver como usar Python para solventarlos.\nPrimero debemos instalar la librería y de la misma llamar a la funcion stats, puedes hacerlo de la siguiente manera:\n\nfrom scipy import stats\n\nListo, ahora veamos algunos casos\n\nCaso de una sola población:\nPongamos el siguiente caso, tomando nustro archivo de emisiones de CO2 y con la media poblacional. Supongamos que Australia cree que la media de emisiones en estos 30 años es de 340000 kt de C02, con esto hacemos nuestra prueba de hipótesis.\n\n\n#Cargamos las librerias \nimport pandas as pd\nimport numpy as np \nfrom scipy import stats\n\n\n# Leemos nuestro archivo de datos de CO2\nruta_archivo_excel = 'data/co2.xlsx'\n\n# Creamos un objeto llamado datos para almacenarlo\ndatos = pd.read_excel(ruta_archivo_excel)\n\n#Seleccionar los dos países\nfila_seleccionada = datos.iloc[9,4:]\n\n\n#Tranformamos\naustralia = np.array(fila_seleccionada) \n\n#Calculamos medidas de dispersion para despues\nmedia_muestra = np.mean(australia)\ndesviacion = np.std(australia)\n\n#Definimos u_0\nu_0 = 340000\n\n#Realizamos el test\n\nresultado = stats.ttest_1samp(australia,u_0)\n\n#comprobamos nuestro valor p\nresultado.pvalue&lt;0.05\n\nComo podemos observar no tenemos evidencia suficiente para rechazar la hipotesis dicha al principio, esto puede ser gracias a la desviacion de los datos, las brechas entre ellos debido al incremento en las industrias en el país, pero sirve para ilustrar cómo realizar pruebas de hipótesis.\nPodemos incluso corroborar el estadístico que obtenemos con el siguiente código\n\n## Hagamos de forma manual para corroborar \nt_prueba=(media_muestra-u_0)/(desviacion/np.sqrt(len(australia)))\n\n#Vemos el estadistico calculado manualmente\nprint(t_prueba)\n\n#Obtenemos el estadítico de la función anterior\nprint(resultado.statistic)\n\n\nSi bien los resultados difieren por decimales esto es normal ya que la librería usa un número mayor de decimales\n\n\nCaso de dos poblaciones:\nTomemos nuestros datos sobre las emiciones de CO2 por cada país, tomemos dos de ellos, Emiratos Árabes y Argentina, y propongamos las siguientes hipotesis\n\n\\(H_0\\) : Ambos países tienen, en promedio, las mismas emisiones de CO2 ( \\(u_1=u_2\\) ).\n\\(H_1\\) : Las medias son distintas.\n\nVeamos como dar una respuesta usando python\n\n\n#Seleccionar los dos países\nfila_seleccionada1 = datos.iloc[5,4:]\nfila_seleccionada2 = datos.iloc[6,4:]\n\n#Tranformamos\nemiratos = np.array(fila_seleccionada1) \nargentina = np.array(fila_seleccionada2)\n\n#buscamos el valor p mencionado\nresultado2 = stats.ttest_ind(emiratos, argentina,equal_var=False)\nprint(resultado2.pvalue)\n#solo en la nueva versión --&gt; print(resultado2.df) grados de libertad\n\nCon ello podemos ver que el valor p es mayor a 0.05, por lo que no rechazamos la hipotesis nula y podriamos considerar que ambos países tienen, en promedio, una emisión igual. Notemos que un parámetro de la función es equal_vareste por defecto lleva el valor “True”, pero si conocemos que son dsitintas podemos cambiarlo.\nIncluso podemos realizar esto de manera manual para comprobar, el código es el siguiente\n\n#medidas que nos ayudaran a simplificar las operaciones\ndesv1 = np.std(emiratos)\nmedia1 = np.mean(emiratos)\ndesv2 = np.std(argentina)\nmedia2 = np.mean(argentina)\nvar1 = desv1**2\nvar2 = desv2**2\n\n#Primero verificamos si las varianzas son iguales\n#Utilizamos el estadítico F de prueba\nF_prueba = desv2**2/desv1**2\nF_prueba\n\n#Limites de rechazo prueba F\nF_est1 = stats.f.ppf(0.975,30,30)\nF_est1\nF_est2 = stats.f.ppf(0.025,30,30)\nF_est2\n\n# Rechazamos la hipotesis de que las varianzas son iguales, por tanto realizamos el análisis tomando en cuenta el tercer caso explicado anteriormente\n\nt_calculado = (media1-media2)/np.sqrt((desv1**2/31)+(desv2**2/31))\nt_calculado\n\n#grados de libertad\nv_arriba = ((var1/31)+(var2/31))**2\nv_abajo = ((var1/31)**2/30)+((var2/31)**2/30)\nv = v_arriba/v_abajo\nv \n\n#limites de rechazo para t\nt_est1= stats.t.ppf(0.025,47) \nt_est1\nt_est2 = stats.t.ppf(0.975,47)\nt_est2\n\n#Vemos que decidimos\nif t_est1&lt;=t_calculado&lt;=t_est2:\n    print(\"No se rechaza H_0\")\nelse:\n    print(\"Se rechaza H_0\")\n\nVeamos un último ejemplo usando lo visto en la clase anterior, agrupando por regiones, en este caso Europa y América Latina. Vamos a responder a la pregunta: ” ¿los paises de Europa emitieron la misma cantidad de co2 que América Latina? ” Por tanto tenemos el código siguiente:\n\n# Creamos un objeto llamado datos2 para almacenarl la hoja con loas regiones\ndatos2 = pd.read_excel(ruta_archivo_excel,sheet_name=1)\n\n#Agrupamos por regiones\neuropa=datos2[datos2[\"Region\"]==\"Europa y Asia central (excluido altos ingresos)\"]\nlatina=datos2[datos2[\"Region\"]==\"América Latina y el Caribe (excluido altos ingresos)\"]\n\n#Unimos solo Europa\ndatos3 = datos.merge(europa, on=[\"Country Name\"],how = \"right\")\n#unimos america latina\ndatos4 = datos.merge(latina, on=[\"Country Name\"],how = \"right\")\n\n#Creamos una lista para ubicar la suma de cada pais\nfila_america=[]\nfila_europa=[]\n\n#Obtenemos esa suma de cada pais por region\nfor i in range(0,23):\n    fila_america.append(np.sum(datos4.iloc[i,4:35]))\nfor j in range(0,20):\n    fila_europa.append(np.sum(datos3.iloc[j,4:35]))\n\n#Creamos dos array\ntot_america=np.array(fila_america)\ntot_europa=np.array(fila_europa)\n\n#Realizamos la prueba de hipotesis\nprueba_hip = stats.ttest_ind(tot_america,tot_europa,equal_var=False)\nprueba_hip\n\nCon esto no podemos rechazar la hipotesis de que ambas regiones tuvieron iguales emisiones en los años.\nSe pueden cambiar más cosas para realizar pruebas de hipótesis (además de ver las funciones de distribución y densidad de algunas distribuciones de probabilidad), se recomienda leer la documentación oficial de Scipy, la cual es la siguiente: https://docs.scipy.org/doc/scipy/reference/stats.html"
  },
  {
    "objectID": "clase_7.html#pruebas-de-normalidad",
    "href": "clase_7.html#pruebas-de-normalidad",
    "title": "Pruebas de hipótesis para la media entre grupos en Python",
    "section": "Pruebas de normalidad",
    "text": "Pruebas de normalidad\nVer si nuestros datos siguen una distribución normal es de suma importancia en la estadística inferencial ya que muchas pruebas asumen que existe una normalidad en los mismos.\nAlgunas pruebas de normalidad son las siguientes:\n\nPrueba gráfica mediante un histograma:\nEn esta prueba queremos observar si nuestros datos sguen una distribución similar a la normal.\nGráfico Q-Q:\nEn este comparamos los cuantiles observados con los esperados de una distribución normal. Si observamos que los puntos en este gráfico se aproximan a una línea diagonal, podemos decir que los datos siguen distribución normal.\nPrueba de Kolmogorov-Smirnov:\nEste compara la distribución acumulativa de los datos con los de una normal.\nPrueba de Shapiro-Wilk:\nEn esta se tiene una prueba de hipótesis donde verificamos si los datos provienen de una distribución normal o no.\n\nVamos a ver algunos ejemplos con la biblioteca Scipy\n\n#Podemos hacer un test de Saphiro-Wilk\ntest_saphiro = stats.shapiro(australia)\ntest_saphiro\n\n#También mostramos cómo ver podemos corroborar con un histograma\nplt.hist(australia)\n\n#Por último otro test es el de D'Agostino's K-squared\ntest_Dangostino = stats.normaltest(australia)\ntest_Dangostino \n\n\nEl test de Saphiro-Wilk no es aconsejado cuando los datos sobrepasan las 50 observaciones."
  },
  {
    "objectID": "clase_8.html",
    "href": "clase_8.html",
    "title": "Análisis de correlación y modelos de regresión en Python",
    "section": "",
    "text": "La regresión lineal como una prueba de diferencia de media\nUsualmente, el valor del estadístico T en los resultados de una regresión lineal OLS (Ordinary Least Squares) y el valor del estadístico T de una prueba t de dos muestras independientes (ttest_ind) no son directamente comparables, ya que se utilizan en contextos diferentes y se calculan de diferentes maneras.\nPara que los resultados de estas dos pruebas sean comparables, tendrías que estar probando exactamente la misma hipótesis en ambos casos, lo cual no es típicamente el caso. Por otro lado, la prueba t de dos muestras independientes también tiene sus propios supuestos, incluyendo la independencia de las observaciones y la igualdad de varianzas en los dos grupos (aunque hay versiones de la prueba que no requieren este último supuesto).\nAhora consideremos el siguiente escenario:\nDebido a que este escenario cumple los mismos supuestos que la regresión lineal podemos emplear esta última como una prueba de hipótesis para la diferencia de medias. Vamos a ver como funciona.\nPrimero con la prueba T para diferencia de medias independientes:\nfrom scipy import stats\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom statsmodels.regression.linear_model import OLS\nimport statsmodels.api as sm\n\n\n# Preparación de datos para una prueba de hipótesis (eliminación de NA)\ndatos = datos[~datos[\"dcronica\"].isna()]\ndatos = datos[~datos[\"n_hijos\"].isna()]\n\n# Creamos los vectores:\ndesnutricion = datos.loc[datos[\"dcronica\"] == 1,\"n_hijos\"]\nno_desnutric = datos.loc[datos[\"dcronica\"] != 1,\"n_hijos\"]\n\n# Fijate en el estadistico T del parametro estimado:\nprueba = stats.ttest_ind(desnutricion,no_desnutric,alternative=\"greater\")\n\nprint(prueba)\n\nTtestResult(statistic=4.210509617301763, pvalue=1.3265622420660088e-05, df=2146.0)\nRecordemos la media de los grupos:\npd.pivot_table(datos, values=\"n_hijos\", index=[ \"dcronica\"], aggfunc=[\"count\", \"sum\", \"mean\", \"max\"])\n\n\n\n\n\n\n\n\ncount\nsum\nmean\nmax\n\n\n\nn_hijos\nn_hijos\nn_hijos\nn_hijos\n\n\ndcronica\n\n\n\n\n\n\n\n\n0.0\n1740\n3909.0\n2.246552\n11.0\n\n\n1.0\n408\n1038.0\n2.544118\n9.0\nAhora con regresión:\nx = datos[\"dcronica\"]\ny = datos[\"n_hijos\"]\n\nest = sm.OLS(y, sm.add_constant(x))\nest2 = est.fit()\n\nprint(est2.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                n_hijos   R-squared:                       0.008\nModel:                            OLS   Adj. R-squared:                  0.008\nMethod:                 Least Squares   F-statistic:                     17.73\nDate:                Sun, 04 Feb 2024   Prob (F-statistic):           2.65e-05\nTime:                        15:04:02   Log-Likelihood:                -3585.2\nNo. Observations:                2148   AIC:                             7174.\nDf Residuals:                    2146   BIC:                             7186.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          2.2466      0.031     72.938      0.000       2.186       2.307\ndcronica       0.2976      0.071      4.211      0.000       0.159       0.436\n==============================================================================\nOmnibus:                      629.392   Durbin-Watson:                   1.987\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             1970.053\nSkew:                           1.476   Prob(JB):                         0.00\nKurtosis:                       6.647   Cond. No.                         2.66\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nFíjate como la media del número de hijos en los hogares en los que viven los niños sin desnutrición crónica se presentan en el termino const (2.2466), mientras que el coeficiente asociado a dcronica es la diferencia de la media del número de hijos en los hogares en que viven los niños con desnutrición crónica menos la media del número de hijos en los hogares en que viven los niños sin  desnutrición crónica (0.2976). Por último observa como el estádistico T de la prueba de diferencia de medias con stats.ttest_ind es el mismo que el valor t asociado al coeficiente dcronica en la tabla de regresión. (4.211)"
  },
  {
    "objectID": "clase_8.html#teorema-de-bayes",
    "href": "clase_8.html#teorema-de-bayes",
    "title": "Análisis de correlación y modelos de regresión en Python",
    "section": "Teorema de Bayes",
    "text": "Teorema de Bayes\nHagamos un ejemplo con Python que nos ayude a visualizar de mejor manera la probabilidad condicional. Empecemos con unos datos, lo que tienes en tus manos es una muestra del 10% de la población presente en la Encuesta Nacional de Desnutrición Infantil (ENDI), trabajaremos con algunas variables:\n\nimport pandas as pd\nimport numpy as np\n\ndatos = pd.read_table(\"data/sample_endi_10p.txt\",delimiter = \"\\t\")\n\ndatos.columns\n\nIndex(['sexo', 'etnia', 'dcronica', 'region', 'n_hijos'], dtype='object')\n\n\nNuestro objetivo será emplear estos datos para calcular la probabilidad de encontrar en nuestra muestra un infante de sexo femenino que tenga desnutrición crónica infantil. Empecemos con una tabla rápida para calcular las proporciones adecuadas:\n\n# Podemos ver los conteos:\npd.crosstab(datos[\"sexo\"],datos[\"dcronica\"])\n\n# E incluir los totales:\npd.crosstab(datos[\"sexo\"],datos[\"dcronica\"], margins = True)\n\n\n\n\n\n\n\ndcronica\n0.0\n1.0\nAll\n\n\nsexo\n\n\n\n\n\n\n\nHombre\n872\n221\n1093\n\n\nMujer\n906\n195\n1101\n\n\nAll\n1778\n416\n2194\n\n\n\n\n\n\n\nAhora calculemos las proporciones:\n\n# Fijense que no estoy usando los margenes ¿Qué pasa si los incluimos?\n\ncruce = pd.crosstab(datos[\"sexo\"],datos[\"dcronica\"])\n\ntotal_global = cruce.sum().sum()\n\nproporcion = pd.crosstab(datos[\"sexo\"],datos[\"dcronica\"],margins = True) /total_global\n\nprint(proporcion)\n\ndcronica       0.0       1.0       All\nsexo                                  \nHombre    0.397448  0.100729  0.498177\nMujer     0.412944  0.088879  0.501823\nAll       0.810392  0.189608  1.000000\n\n\nSi volvemos a nuestro ejercicio, consideramos a \\(B\\) como el evento de que se seleccione un infante de sexo femenino, y \\(A\\) como el evento en que el infante seleccionado tiene desnutrición crónica (valor igual a 1) tendriamos lo siguiente:\n\nPrimero para el evento dado: \\[P(B) =  P(B \\cap A) + P(B \\cap A^c) = 0.412.. + 0.089.. = 0.501823\\] Luego vemos a que \\(P(A \\cap B) = 0.088879\\)\nEntonces la probabilidad condicional \\[P(A|B) = \\frac{P(A \\cap B)}{P(B)} = 0.177111\\]\n\nPero que tal si ahora le damos la vuelta al ejercicio ¿Cual sería la probabilidad de que habiendo elegido un niño al azar sin desnutrición crónica, este resulte ser mujer?\n\nPrimero para el evento dado: \\[P(A) =  P(A \\cap B) + P(A \\cap B^c) = 0.189608\\] Luego vemos a que \\(P(A \\cap B) = 0.088879\\)$\nEntonces la probabilidad condicional \\[P(A|B) = \\frac{P(A \\cap B)}{P(B)} = 0.468751\\]\n\nInteresante verdad, en este caso importa de que población estamos hablando a la hora de tomar las proporciones.\nAhora veamos con Python como podemos hacer los cálculos de variables agrupadas en este estilo de forma adecuada, pero en este caso vamos a emplear funciones lambda para calcular el numero de hijos promedio en los hogares de los con y sin desnutrición infantil:\n\npivot_table = pd.pivot_table(datos, values='n_hijos', index=['sexo', 'dcronica'], \n                             aggfunc={'n_hijos': [ 'mean', \n                                            (\"casos\", lambda x: x.count()),\n                                            (\"proporcion\", lambda x: x.count()/len(datos))\n                                            ]})\n\n\nConceptos Clave de la Regresión Lineal\nPara entender la fórmula de la esperanza condicional de \\(Y\\) dado \\(X=x\\), primero debemos comprender qué significa en el contexto de la probabilidad y la estadística. La esperanza condicional, o el valor esperado condicional, es una medida de tendencia central para una variable aleatoria (Y) dado que otra variable aleatoria (X) toma un valor específico \\(x\\). En el marco de la regresión lineal, la esperanza condicional nos ayuda a predecir el valor medio esperado de (Y) para un valor dado de (X), basándonos en la relación lineal entre estas dos variables.\nLa fórmula general para la esperanza condicional de (Y) dado (X=x) se expresa mediante la integral de (y) veces la función de densidad condicional de \\(Y\\) dado \\(X=x\\), sobre todos los posibles valores de $y4. Matemáticamente, esto se representa como:\n\\[ E[Y|X=x] = \\int y \\cdot f_{Y|X}(y|x) \\, dy \\]\nDonde:\n\n\\(E[Y|X=x]\\) es la esperanza condicional de (Y) dado (X=x).\n\\(f_{Y|X}(y|x)\\) es la función de densidad de probabilidad condicional de (Y) dado (X=x).\nLa integral se calcula sobre todo el rango de posibles valores de (y).\nRegresión Lineal: Es una técnica estadística utilizada para estudiar la relación entre dos variables continuas. En este caso, estamos mirando cómo las compras (variable independiente x) se relacionan con las ventas (variable dependiente y).\nEsperanza Condicionada: Se refiere a la expectativa de una variable aleatoria dado el valor de otra. En la regresión lineal, buscamos la expectativa de y dado un valor específico de x.\n\nLa regresión lineal es una herramienta poderosa para entender cómo una variable dependiente y está relacionada con una variable independiente x. Usamos una línea (la más “ajustada” posible) para describir esta relación. La ecuación general de una línea es:\n\\[y = \\alpha + \\beta x + \\epsilon \\]\nDonde:\n\n(\\(y\\)) es la variable dependiente (en nuestro caso, vamos a emplear las ventas de empresas de una misma industria).\n(\\(x\\)) es la variable independiente (en este caso, compras de las empresas).\n(\\(\\alpha\\)) es la intersección con el eje Y o el valor de y cuando (x = 0). Representa el valor base de ventas sin ninguna compra.\n(\\(\\beta\\)) es la pendiente de la línea, que indica cuánto cambia y por cada unidad de cambio en x.\n\n\n\nEstimación de los Coeficientes\nLa regresión lineal intenta encontrar los valores de () y () que minimizan la suma de los cuadrados de los errores (()) entre los valores observados de y y los valores predichos por nuestra ecuación lineal. Esto se conoce como el método de mínimos cuadrados. Las fórmulas para calcular () y () son:\n\\[\\beta = \\frac{\\sum_{i=1}^{n} (x_i - \\overline{x})(y_i - \\overline{y})}{\\sum_{i=1}^{n} (x_i - \\overline{x})^2} \\]\n\\[ \\alpha = \\overline{y} - \\beta\\overline{x} \\]\nDonde:\n\n\\(x_i\\) y \\(y_i\\) son los valores individuales de x y y.\n\\(\\overline{x}\\) y \\(\\overline{y}\\) son los promedios de x y y, respectivamente.\n\\(n\\) es el número total de observaciones.\n\nEs importante aclarar que (\\(\\alpha\\)) en sí mismo no es “la media” de y, pero sí representa el punto donde la línea de regresión cruza el eje Y, es decir, el valor esperado de y cuando x es 0. Sin embargo, si centramos nuestra atención en la ecuación de (\\(\\alpha\\)), podemos ver cómo está influenciada por los promedios de x y y a través de (\\(\\beta\\)).\nLa regresión lineal se puede entender como la estimación de la esperanza condicional de y dado x, \\(E[y|x]\\). Esto significa que para cualquier valor de x, la regresión lineal nos da el mejor pronóstico, en promedio, de y. En otras palabras, nos dice qué valor esperar para y basándonos en el valor de x, teniendo en cuenta la relación lineal identificada entre estas dos variables."
  },
  {
    "objectID": "clase_8.html#vamos-a-ver-la-regresión-lineal-en-todo-su-poder",
    "href": "clase_8.html#vamos-a-ver-la-regresión-lineal-en-todo-su-poder",
    "title": "Análisis de correlación y modelos de regresión en Python",
    "section": "Vamos a ver la regresión lineal en todo su poder:",
    "text": "Vamos a ver la regresión lineal en todo su poder:\nPrimero, vamos a crear algunos datos “perfectos” para entender cómo funciona la regresión lineal. Ya te explicaremos por que “perfectos”.\nnp.random.seed(1984) # Para reproducibilidad\nx = np.random.rand(100) * 10 # 100 puntos de compra\nalpha = 3.5 # La media\nerror = np.random.randn(100) # Error normalmente distribuido\ny = alpha + 1.7 * x + error # Ventas simuladas\nEn este fragmento de código:\n\nnp.random.seed(1984): Esto asegura que cada vez que ejecutes el código, los números aleatorios generados sean los mismos. Es útil para reproducibilidad.\nx: Representa nuestras variables independientes, en este caso, puntos de compra. Imagínalo como la cantidad de dinero gastado en publicidad.\nalpha y error: alpha es el término de intercepción en nuestra ecuación lineal, y error añade variabilidad aleatoria a nuestros datos, simulando la impredecibilidad del mundo real.\ny: Es nuestra variable dependiente, representando las ventas. Está calculada a partir de una relación lineal con x más algún error aleatorio.\n\nAhora, ajustaremos un modelo de regresión lineal a nuestros datos simulados.\nmodel = LinearRegression()\nresultado = model.fit(x.reshape(-1, 1), y) # Ajustar el modelo\nAquí, LinearRegression() crea un nuevo modelo de regresión lineal, y model.fit() ajusta este modelo a nuestros datos. La parte x.reshape(-1, 1) simplemente reforma x para que tenga la forma correcta (una columna de valores en lugar de una lista plana), lo cual es necesario para el método fit().\nVamos a ver qué tan bien nuestro modelo se ajustó a los datos.\nprint(f\"Alpha real: {alpha}, Alpha estimado: {model.intercept_}\")\nprint(f\"Coeficiente real: 1.7, Coeficiente estimado: {model.coef_[0]}\")\nEsto nos mostrará el valor de alpha y el coeficiente que nuestro modelo estimó en comparación con los valores reales que usamos para generar los datos.\nFinalmente, visualizamos nuestros datos simulados y la línea de regresión ajustada.\nplt.scatter(x, y, color='blue', label='Datos Simulados')\nplt.plot(x, model.predict(x.reshape(-1, 1)), color='red', label='Línea de Regresión')\nplt.xlabel('Compras')\nplt.ylabel('Ventas')\nplt.legend()\nplt.show()\nEsto nos da una representación visual clara de cómo la línea de regresión se ajusta a los datos simulados.\n\nSupuestos de Normalidad en OLS\nCuando usamos Mínimos Cuadrados Ordinarios (OLS, por sus siglas en inglés) para ajustar una regresión lineal, asumimos que los errores (error en nuestro ejemplo) siguen una distribución normal. Esto es crucial para la inferencia estadística posterior, como calcular intervalos de confianza y realizar pruebas de hipótesis. En nuestro ejemplo simulado, respetamos este supuesto al generar error con np.random.randn(100), que simula errores normalmente distribuidos.\n\n\nLa Realidad de los Datos en el Mundo Real\nAunque este ejemplo nos proporciona una comprensión clara de cómo funciona la regresión lineal, en el mundo real, los datos raramente son tan “limpios”. Los verdaderos conjuntos de datos suelen tener errores no normales, valores atípicos, y a menudo no siguen una relación lineal tan nítida. Por eso, es crucial realizar un análisis exploratorio de datos (EDA, por sus siglas en inglés) antes de ajustar cualquier modelo y considerar transformaciones de datos o modelos más complejos si es necesario."
  },
  {
    "objectID": "clase_8.html#supuestos-de-la-regresión-lineal",
    "href": "clase_8.html#supuestos-de-la-regresión-lineal",
    "title": "Análisis de correlación y modelos de regresión en Python",
    "section": "Supuestos de la regresión lineal",
    "text": "Supuestos de la regresión lineal\nAdemás, la regresión lineal OLS tiene una serie de supuestos que deben cumplirse para que los resultados sean válidos, incluyendo:\n\nLinealidad: La relación entre las variables independientes y la variable dependiente es lineal.\nIndependencia: Las observaciones son independientes entre sí.\nHomoscedasticidad: La varianza de los errores es constante a lo largo de todas las observaciones.\nNormalidad: Los errores están normalmente distribuidos."
  }
]